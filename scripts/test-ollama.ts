/**
 * Script para probar Ollama como fallback
 */

// Cargar variables de entorno
import dotenv from 'dotenv'
import path from 'path'

// Cargar .env desde la ra√≠z del proyecto
dotenv.config({ path: path.resolve(process.cwd(), '.env') })

import { AIMultiProvider } from '@/lib/ai-multi-provider'

async function testOllama() {
  console.log('üß™ Probando Ollama como Fallback\n')
  console.log('='.repeat(50))

  // Test 1: Verificar configuraci√≥n
  console.log('\nüìã Test 1: Configuraci√≥n')
  console.log('OLLAMA_BASE_URL:', process.env.OLLAMA_BASE_URL || '‚ùå No configurado')
  console.log('OLLAMA_MODEL:', process.env.OLLAMA_MODEL || '‚ùå No configurado')
  console.log('OLLAMA_ENABLED:', process.env.OLLAMA_ENABLED || '‚ùå No configurado')
  console.log('OLLAMA_TIMEOUT:', process.env.OLLAMA_TIMEOUT || '‚ùå No configurado')
  console.log('AI_FALLBACK_ORDER:', process.env.AI_FALLBACK_ORDER || '‚ùå No configurado')
  console.log('GROQ_API_KEY:', process.env.GROQ_API_KEY ? '‚úÖ Configurado' : '‚ùå No configurado')

  // Test 2: Probar respuesta simple
  console.log('\n\nüí¨ Test 2: Respuesta Simple')
  console.log('='.repeat(50))

  const testMessages = [
    {
      role: 'system' as const,
      content: 'Eres un asistente de ventas. Responde en espa√±ol, m√°ximo 3 l√≠neas.'
    },
    {
      role: 'user' as const,
      content: 'Tienes laptops?'
    }
  ]

  try {
    console.log('\nüë§ Cliente: "Tienes laptops?"')
    console.log('‚è≥ Esperando respuesta de Ollama...')
    
    const startTime = Date.now()
    
    const response = await AIMultiProvider.generateCompletion(testMessages, {
      max_tokens: 200,
      temperature: 0.7
    })
    
    const endTime = Date.now()
    const duration = endTime - startTime

    console.log(`\n‚úÖ Respuesta recibida en ${duration}ms (${(duration/1000).toFixed(1)}s)`)
    console.log(`ü§ñ Provider: ${response.provider}`)
    console.log(`üì¶ Modelo: ${response.model}`)
    console.log(`\nüí¨ Respuesta:\n${response.content}`)

    // Verificar si us√≥ Ollama
    if (response.provider === 'ollama') {
      console.log('\n‚úÖ Ollama funcionando correctamente!')
    } else {
      console.log(`\n‚ö†Ô∏è  Se us√≥ ${response.provider} en lugar de Ollama`)
      console.log('   Esto es normal si Groq/OpenRouter est√°n disponibles')
    }

  } catch (error: any) {
    console.error('\n‚ùå Error:', error.message)
    console.error('\nüîß Posibles soluciones:')
    console.error('   1. Verificar que Ollama est√© corriendo')
    console.error('   2. Verificar OLLAMA_BASE_URL en .env')
    console.error('   3. Verificar que el modelo est√© descargado')
    console.error('   4. Aumentar OLLAMA_TIMEOUT')
  }

  // Test 3: Forzar uso de Ollama
  console.log('\n\nüéØ Test 3: Forzar Uso de Ollama')
  console.log('='.repeat(50))

  try {
    // Temporalmente cambiar el orden de fallback
    const originalOrder = process.env.AI_FALLBACK_ORDER
    process.env.AI_FALLBACK_ORDER = 'ollama'

    console.log('\nüë§ Cliente: "Cu√°nto cuesta?"')
    console.log('‚è≥ Forzando uso de Ollama...')
    
    const startTime = Date.now()
    
    const response = await AIMultiProvider.generateCompletion([
      {
        role: 'system' as const,
        content: 'Eres un asistente. Responde en espa√±ol, m√°ximo 2 l√≠neas.'
      },
      {
        role: 'user' as const,
        content: 'Cu√°nto cuesta?'
      }
    ], {
      max_tokens: 150,
      temperature: 0.7
    })
    
    const endTime = Date.now()
    const duration = endTime - startTime

    console.log(`\n‚úÖ Respuesta en ${duration}ms (${(duration/1000).toFixed(1)}s)`)
    console.log(`ü§ñ Provider: ${response.provider}`)
    console.log(`üí¨ Respuesta:\n${response.content}`)

    // Restaurar orden original
    process.env.AI_FALLBACK_ORDER = originalOrder

    if (response.provider === 'ollama') {
      console.log('\n‚úÖ Ollama funciona correctamente como fallback!')
    }

  } catch (error: any) {
    console.error('\n‚ùå Error forzando Ollama:', error.message)
  }

  // Resumen
  console.log('\n\nüìä Resumen')
  console.log('='.repeat(50))
  console.log('\n‚úÖ Configuraci√≥n de Fallback:')
  console.log('   1. Groq (r√°pido, l√≠mite de tokens)')
  console.log('   2. Ollama (ilimitado, m√°s lento)')
  console.log('\nüí° Ollama se usar√° cuando:')
  console.log('   - Groq se quede sin tokens')
  console.log('   - Groq falle por cualquier raz√≥n')
  console.log('   - Necesites respuestas ilimitadas 24/7')
  console.log('\n‚è±Ô∏è  Tiempos esperados:')
  console.log('   - Groq: 1-3 segundos')
  console.log('   - Ollama: 10-30 segundos')
  console.log('\nüöÄ El bot nunca dejar√° de funcionar!')
}

// Ejecutar pruebas
testOllama().catch(console.error)

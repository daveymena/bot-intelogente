// üß™ Script para probar SOLO LM Studio

import { config } from 'dotenv'
config()

console.log('üéÆ PRUEBA DE LM STUDIO\n')
console.log('=' .repeat(60))

const LM_STUDIO_URL = process.env.LM_STUDIO_URL || 'http://localhost:1234/v1/chat/completions'
const LM_STUDIO_MODEL = process.env.LM_STUDIO_MODEL || 'phi-2'

console.log('\nüìã Configuraci√≥n:')
console.log('‚îÄ'.repeat(60))
console.log(`URL: ${LM_STUDIO_URL}`)
console.log(`Modelo: ${LM_STUDIO_MODEL}`)

async function testLMStudio() {
  console.log('\n\nüîç Paso 1: Verificar que LM Studio est√© ejecut√°ndose...\n')
  
  try {
    // Probar endpoint de modelos
    console.log('üì° Conectando a LM Studio...')
    const modelsUrl = LM_STUDIO_URL.replace('/chat/completions', '/models')
    
    const modelsResponse = await fetch(modelsUrl, {
      method: 'GET',
      signal: AbortSignal.timeout(5000)
    })
    
    if (!modelsResponse.ok) {
      throw new Error(`HTTP ${modelsResponse.status}: ${modelsResponse.statusText}`)
    }
    
    const modelsData = await modelsResponse.json()
    console.log('‚úÖ LM Studio est√° ejecut√°ndose!')
    console.log(`üì¶ Modelos disponibles: ${modelsData.data?.length || 0}`)
    
    if (modelsData.data && modelsData.data.length > 0) {
      console.log('\nüìã Lista de modelos:')
      modelsData.data.forEach((model: any, index: number) => {
        console.log(`   ${index + 1}. ${model.id}`)
      })
    }
    
  } catch (error: any) {
    console.error('‚ùå Error conectando a LM Studio:', error.message)
    console.log('\nüí° Soluciones:')
    console.log('   1. Abre LM Studio')
    console.log('   2. Ve a Settings (‚öôÔ∏è)')
    console.log('   3. Activa "Enable local REST API server"')
    console.log('   4. Verifica que el puerto sea 1234')
    console.log('   5. Carga un modelo en la pesta√±a Chat')
    console.log('\n‚ö†Ô∏è  No se puede continuar sin LM Studio ejecut√°ndose')
    process.exit(1)
  }
  
  console.log('\n\nü§ñ Paso 2: Probar generaci√≥n de respuesta...\n')
  
  const testMessages = [
    {
      role: 'system',
      content: 'Responde brevemente en espa√±ol.'
    },
    {
      role: 'user',
      content: 'Di hola'
    }
  ]
  
  console.log('üì§ Enviando mensaje simple: "Di hola"')
  console.log('‚è≥ Esperando respuesta de LM Studio...\n')
  
  try {
    const startTime = Date.now()
    
    const response = await fetch(LM_STUDIO_URL, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: LM_STUDIO_MODEL,
        messages: testMessages,
        temperature: 0.7,
        max_tokens: 50,
        stream: false
      }),
      signal: AbortSignal.timeout(60000) // 60 segundos
    })
    
    const endTime = Date.now()
    const duration = ((endTime - startTime) / 1000).toFixed(2)
    
    if (!response.ok) {
      const errorText = await response.text()
      throw new Error(`HTTP ${response.status}: ${errorText}`)
    }
    
    const data = await response.json()
    const content = data.choices?.[0]?.message?.content
    
    if (!content) {
      throw new Error('LM Studio no devolvi√≥ contenido en la respuesta')
    }
    
    console.log('‚úÖ RESPUESTA RECIBIDA:')
    console.log('‚îÄ'.repeat(60))
    console.log(`‚è±Ô∏è  Tiempo: ${duration} segundos`)
    console.log(`üì¶ Modelo usado: ${data.model || LM_STUDIO_MODEL}`)
    console.log(`\nüí¨ Contenido:\n${content}`)
    console.log('‚îÄ'.repeat(60))
    
  } catch (error: any) {
    console.error('\n‚ùå Error generando respuesta:', error.message)
    
    if (error.name === 'TimeoutError' || error.message.includes('timeout')) {
      console.log('\nüí° El modelo est√° tardando mucho:')
      console.log('   1. Usa un modelo m√°s peque√±o (phi-2 recomendado)')
      console.log('   2. Verifica que el modelo est√© cargado en LM Studio')
      console.log('   3. Cierra otros programas pesados')
    } else {
      console.log('\nüí° Posibles soluciones:')
      console.log('   1. Verifica que el modelo est√© cargado en LM Studio')
      console.log('   2. Ve a la pesta√±a Chat y selecciona un modelo')
      console.log('   3. Espera a que el modelo termine de cargar')
    }
    process.exit(1)
  }
  
  console.log('\n\nüîÑ Paso 3: Probar m√∫ltiples respuestas...\n')
  
  const testQuestions = [
    '¬øCu√°nto cuesta?',
    'Dame m√°s informaci√≥n',
    'Gracias'
  ]
  
  for (let i = 0; i < testQuestions.length; i++) {
    const question = testQuestions[i]
    console.log(`\nüì§ Pregunta ${i + 1}: "${question}"`)
    
    try {
      const startTime = Date.now()
      
      const response = await fetch(LM_STUDIO_URL, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: LM_STUDIO_MODEL,
          messages: [
            { role: 'system', content: 'Responde brevemente.' },
            { role: 'user', content: question }
          ],
          temperature: 0.7,
          max_tokens: 100,
          stream: false
        }),
        signal: AbortSignal.timeout(20000)
      })
      
      const endTime = Date.now()
      const duration = ((endTime - startTime) / 1000).toFixed(2)
      
      if (response.ok) {
        const data = await response.json()
        const content = data.choices?.[0]?.message?.content
        console.log(`   ‚úÖ Respondi√≥ en ${duration}s: "${content?.substring(0, 50)}..."`)
      } else {
        console.log(`   ‚ùå Error HTTP ${response.status}`)
      }
      
    } catch (error: any) {
      console.log(`   ‚ùå Error: ${error.message}`)
    }
  }
  
  console.log('\n\n' + '='.repeat(60))
  console.log('‚úÖ PRUEBA COMPLETADA')
  console.log('='.repeat(60))
  
  console.log('\nüìä RESUMEN:')
  console.log('   ‚úÖ LM Studio est√° funcionando correctamente')
  console.log('   ‚úÖ Puede generar respuestas')
  console.log('   ‚úÖ Listo para usar como respaldo')
  
  console.log('\nüí° PR√ìXIMOS PASOS:')
  console.log('   1. Mant√©n LM Studio ejecut√°ndose')
  console.log('   2. El bot usar√° LM Studio autom√°ticamente si Groq falla')
  console.log('   3. Ejecuta: npm run dev')
  
  console.log('\nüéâ ¬°Tu bot ahora tiene respaldo local sin l√≠mites!')
}

// Ejecutar prueba
testLMStudio()
  .then(() => process.exit(0))
  .catch((error) => {
    console.error('\n‚ùå Error fatal:', error)
    process.exit(1)
  })

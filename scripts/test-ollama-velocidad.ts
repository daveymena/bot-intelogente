/**
 * Test de velocidad de Ollama
 * Verifica que Ollama est√© funcionando r√°pido y correctamente
 */

import { AIMultiProvider } from '../src/lib/ai-multi-provider';

async function testOllamaVelocidad() {
  console.log('üöÄ Test de Velocidad de Ollama\n');
  console.log('='.repeat(60) + '\n');

  // Configuraci√≥n - Cargar .env
  require('dotenv').config();
  
  const ollamaUrl = process.env.OLLAMA_BASE_URL || process.env.OLLAMA_URL || 'http://localhost:11434';
  const model = process.env.OLLAMA_MODEL || 'gemma:2b';

  console.log(`üìç URL: ${ollamaUrl}`);
  console.log(`ü§ñ Modelo: ${model}`);
  console.log(`‚è±Ô∏è  Timeout: ${process.env.OLLAMA_TIMEOUT || '10000'}ms\n`);

  // Test 1: Verificar conexi√≥n
  console.log('1Ô∏è‚É£ Verificando conexi√≥n con Ollama...');
  try {
    const response = await fetch(`${ollamaUrl}/api/tags`);
    if (response.ok) {
      const data = await response.json();
      console.log(`   ‚úÖ Ollama conectado`);
      console.log(`   üì¶ Modelos disponibles: ${data.models?.length || 0}`);
      
      if (data.models) {
        data.models.forEach((m: any) => {
          console.log(`      - ${m.name} (${(m.size / 1024 / 1024 / 1024).toFixed(2)} GB)`);
        });
      }
    } else {
      throw new Error(`HTTP ${response.status}`);
    }
  } catch (error: any) {
    console.error(`   ‚ùå Error: ${error.message}`);
    console.error('\nüí° Verifica que Ollama est√© corriendo y accesible\n');
    return;
  }

  console.log('');

  // Test 2: Respuesta simple (velocidad)
  console.log('2Ô∏è‚É£ Test de velocidad - Respuesta simple...');
  try {
    const startTime = Date.now();
    
    const response = await AIMultiProvider.generateCompletion([
      {
        role: 'system',
        content: 'Eres un asistente de ventas conciso y directo.'
      },
      {
        role: 'user',
        content: 'Hola'
      }
    ], {
      max_tokens: 50
    });

    const responseTime = Date.now() - startTime;
    
    console.log(`   ‚ö° Tiempo de respuesta: ${responseTime}ms`);
    console.log(`   ü§ñ Provider: ${response.provider}`);
    console.log(`   üìù Respuesta: "${response.content.slice(0, 100)}..."`);
    
    if (responseTime < 2000) {
      console.log(`   ‚úÖ EXCELENTE - Muy r√°pido!`);
    } else if (responseTime < 5000) {
      console.log(`   ‚úÖ BUENO - Velocidad aceptable`);
    } else {
      console.log(`   ‚ö†Ô∏è LENTO - Considera optimizar`);
    }
  } catch (error: any) {
    console.error(`   ‚ùå Error: ${error.message}`);
  }

  console.log('');

  // Test 3: Respuesta de producto (caso real)
  console.log('3Ô∏è‚É£ Test de caso real - Consulta de producto...');
  try {
    const startTime = Date.now();
    
    const response = await AIMultiProvider.generateCompletion([
      {
        role: 'system',
        content: 'Eres un asistente de ventas de tecnolog√≠a. Responde de forma breve y √∫til.'
      },
      {
        role: 'user',
        content: '¬øTienes laptops disponibles?'
      }
    ], {
      max_tokens: 150
    });

    const responseTime = Date.now() - startTime;
    
    console.log(`   ‚ö° Tiempo de respuesta: ${responseTime}ms`);
    console.log(`   ü§ñ Provider: ${response.provider}`);
    console.log(`   üìù Respuesta: "${response.content}"`);
    
    if (responseTime < 3000) {
      console.log(`   ‚úÖ EXCELENTE - Cliente no esperar√° mucho`);
    } else if (responseTime < 7000) {
      console.log(`   ‚úÖ ACEPTABLE - Velocidad razonable`);
    } else {
      console.log(`   ‚ö†Ô∏è LENTO - Cliente puede impacientarse`);
    }
  } catch (error: any) {
    console.error(`   ‚ùå Error: ${error.message}`);
  }

  console.log('');

  // Test 4: M√∫ltiples requests (carga)
  console.log('4Ô∏è‚É£ Test de carga - 5 requests consecutivos...');
  const times: number[] = [];
  
  for (let i = 1; i <= 5; i++) {
    try {
      const startTime = Date.now();
      
      await AIMultiProvider.generateCompletion([
        {
          role: 'system',
          content: 'Responde en una palabra.'
        },
        {
          role: 'user',
          content: `Test ${i}`
        }
      ], {
        max_tokens: 10
      });

      const responseTime = Date.now() - startTime;
      times.push(responseTime);
      
      console.log(`   Request ${i}/5: ${responseTime}ms`);
      
      // Peque√±o delay entre requests
      await new Promise(resolve => setTimeout(resolve, 100));
    } catch (error: any) {
      console.error(`   Request ${i}/5: ‚ùå ${error.message}`);
    }
  }

  if (times.length > 0) {
    const avgTime = times.reduce((a, b) => a + b, 0) / times.length;
    const minTime = Math.min(...times);
    const maxTime = Math.max(...times);
    
    console.log(`\n   üìä Estad√≠sticas:`);
    console.log(`      Promedio: ${avgTime.toFixed(0)}ms`);
    console.log(`      M√≠nimo: ${minTime}ms`);
    console.log(`      M√°ximo: ${maxTime}ms`);
    
    if (avgTime < 2000) {
      console.log(`      ‚úÖ Rendimiento EXCELENTE bajo carga`);
    } else if (avgTime < 4000) {
      console.log(`      ‚úÖ Rendimiento BUENO bajo carga`);
    } else {
      console.log(`      ‚ö†Ô∏è Rendimiento MEJORABLE bajo carga`);
    }
  }

  console.log('\n' + '='.repeat(60));
  console.log('\n‚ú® Test completado!\n');

  // Recomendaciones
  console.log('üí° Recomendaciones para optimizar velocidad:\n');
  console.log('1. Usa modelos peque√±os para respuestas r√°pidas:');
  console.log('   - gemma:2b (2GB) - MUY R√ÅPIDO ‚ö°');
  console.log('   - phi:2.7b (2.7GB) - R√ÅPIDO');
  console.log('   - llama3.2:3b (3GB) - BALANCEADO\n');
  
  console.log('2. Ajusta max_tokens seg√∫n necesidad:');
  console.log('   - Saludos: 50 tokens');
  console.log('   - Respuestas cortas: 150 tokens');
  console.log('   - Respuestas detalladas: 300 tokens\n');
  
  console.log('3. Configura timeout apropiado:');
  console.log('   - OLLAMA_TIMEOUT=10000 (10 segundos)\n');
  
  console.log('4. Usa GPU si est√° disponible en tu servidor\n');
  
  console.log('5. Considera fallback a Groq para respuestas complejas\n');
}

testOllamaVelocidad().catch(console.error);

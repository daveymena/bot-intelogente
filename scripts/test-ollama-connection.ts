import { AIMultiProvider } from '../src/lib/ai-multi-provider'

async function testOllamaConnection() {
  console.log('üß™ Probando conexi√≥n con Ollama...\n')

  try {
    // Probar usando el sistema completo de fallback
    console.log('üîÑ Probando sistema completo de IA (Groq ‚Üí Ollama)...')
    const response = await AIMultiProvider.generateCompletion([
      { role: 'system', content: 'Eres un asistente √∫til.' },
      { role: 'user', content: 'Di "Ollama funciona correctamente" si me entiendes.' }
    ], { max_tokens: 50 })

    if (response && response.success) {
      console.log('‚úÖ Sistema de IA funciona correctamente!')
      console.log('üìÑ Respuesta:', response.content)
      console.log('ü§ñ Modelo usado:', response.model)
      console.log('üè∑Ô∏è  Provider:', response.provider)

      if (response.provider === 'ollama') {
        console.log('üéâ ¬°Ollama est√° funcionando como provider!')
      } else {
        console.log(`üìù Usando ${response.provider} como provider principal`)
      }
    } else {
      console.log('‚ùå El sistema de IA no respondi√≥ correctamente')
      console.log('Detalles:', response)
    }
  } catch (error) {
    console.error('‚ùå Error en el sistema de IA:')
    console.error((error as Error).message)

    if ((error as Error).message.includes('fetch failed')) {
      console.log('\nüí° Posibles causas:')
      console.log('1. Ollama no est√° ejecut√°ndose')
      console.log('2. La URL de Ollama es incorrecta')
      console.log('3. El modelo no est√° descargado')
      console.log('4. Problemas de red/conectividad')
    } else if ((error as Error).message.includes('timeout')) {
      console.log('\nüí° Posibles causas:')
      console.log('1. Ollama est√° tardando mucho en responder')
      console.log('2. El timeout est√° muy bajo')
      console.log('3. El modelo es muy grande y tarda en cargar')
    } else if ((error as Error).message.includes('Todas las APIs')) {
      console.log('\nüí° Todas las APIs de IA fallaron')
      console.log('1. Verifica que Ollama est√© ejecut√°ndose')
      console.log('2. Verifica las credenciales de Groq')
      console.log('3. Revisa la conectividad de red')
    }
  }

  console.log('\nüîç Informaci√≥n de configuraci√≥n:')
  console.log('AI_FALLBACK_ORDER:', process.env.AI_FALLBACK_ORDER || 'groq,ollama')
  console.log('OLLAMA_BASE_URL:', process.env.OLLAMA_BASE_URL || 'https://bot-whatsapp-ollama.sqaoeo.easypanel.host')
  console.log('OLLAMA_MODEL:', process.env.OLLAMA_MODEL || 'gemma:2b')
  console.log('OLLAMA_ENABLED:', process.env.OLLAMA_ENABLED || 'true')
  console.log('GROQ_API_KEY:', process.env.GROQ_API_KEY ? '‚úÖ Configurado' : '‚ùå No configurado')

  // Probar Ollama directamente sin el sistema de fallback
  console.log('\nüîÑ Probando Ollama directamente...')
  try {
    const ollamaUrl = process.env.OLLAMA_BASE_URL || 'https://bot-whatsapp-ollama.sqaoeo.easypanel.host'
    const response = await fetch(`${ollamaUrl}/api/tags`, {
      method: 'GET',
      headers: {
        'Content-Type': 'application/json'
      }
    })

    if (response.ok) {
      const data = await response.json()
      console.log('‚úÖ Ollama est√° ejecut√°ndose!')
      console.log('üìã Modelos disponibles:', data.models?.map((m: any) => m.name).join(', ') || 'Ninguno')
    } else {
      console.log('‚ùå Ollama responde pero con error:', response.status)
    }
  } catch (directError) {
    console.log('‚ùå Ollama no est√° accesible directamente')
    console.log('Error:', (directError as Error).message)
  }

  // Probar generaci√≥n de respuesta con Ollama
  console.log('\nü§ñ Probando generaci√≥n de respuesta con Ollama...')
  try {
    const ollamaUrl = process.env.OLLAMA_BASE_URL || 'https://bot-whatsapp-ollama.sqaoeo.easypanel.host'
    const chatResponse = await fetch(`${ollamaUrl}/api/chat`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'gemma:2b',
        messages: [
          { role: 'system', content: 'Eres un asistente √∫til para un negocio de tecnolog√≠a.' },
          { role: 'user', content: 'Hola, ¬øqu√© productos ofrecen?' }
        ],
        stream: false,
        options: {
          temperature: 0.7,
          num_predict: 100
        }
      })
    })

    if (chatResponse.ok) {
      const chatData = await chatResponse.json()
      console.log('‚úÖ Ollama gener√≥ respuesta exitosamente!')
      console.log('üìù Respuesta:', chatData.message?.content || 'Sin contenido')
      console.log('‚ö° Tiempo estimado:', chatData.total_duration ? `${(chatData.total_duration / 1e9).toFixed(2)}s` : 'N/A')
    } else {
      console.log('‚ùå Error en generaci√≥n:', chatResponse.status)
      const errorText = await chatResponse.text()
      console.log('Detalles:', errorText)
    }
  } catch (chatError) {
    console.log('‚ùå Error probando chat con Ollama')
    console.log('Error:', (chatError as Error).message)
  }
}

// Ejecutar el test
testOllamaConnection().catch(console.error)
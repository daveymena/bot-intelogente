/**
 * Test del Bot HÃ­brido
 * Demuestra cÃ³mo funciona el sistema: Bot Local + Ollama Assistant
 */

import { HybridBotService } from './src/lib/hybrid-bot-service';

const testCases = [
  {
    name: 'Saludo simple',
    message: 'Hola',
    expectedSource: 'local',
    description: 'Bot local responde instantÃ¡neamente'
  },
  {
    name: 'Consulta de mÃ©todos de pago',
    message: 'Â¿CÃ³mo puedo pagar?',
    expectedSource: 'local',
    description: 'Bot local tiene respuesta predefinida'
  },
  {
    name: 'BÃºsqueda de producto',
    message: 'Necesito una laptop para diseÃ±o grÃ¡fico',
    expectedSource: 'hybrid',
    description: 'Ollama analiza intenciÃ³n + busca productos'
  },
  {
    name: 'Consulta compleja',
    message: 'Busco un computador econÃ³mico pero que sea bueno para editar videos',
    expectedSource: 'ollama',
    description: 'Ollama interpreta requisitos complejos'
  },
  {
    name: 'Seguimiento con contexto',
    message: 'Â¿Y ese cuÃ¡nto cuesta?',
    expectedSource: 'ollama',
    description: 'Ollama usa memoria del contexto previo'
  },
  {
    name: 'Agradecimiento',
    message: 'Muchas gracias',
    expectedSource: 'local',
    description: 'Bot local responde rÃ¡pido'
  }
];

async function runTest() {
  console.log(`
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    TEST DEL BOT HÃBRIDO                                    â•‘
â•‘                                                                            â•‘
â•‘  Bot Local: Respuestas rÃ¡pidas predefinidas (instantÃ¡neo)                 â•‘
â•‘  Ollama: InterpretaciÃ³n inteligente y contexto (23s promedio)             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  `);

  // Verificar disponibilidad
  const stats = await HybridBotService.getStats();
  console.log('ğŸ“Š Estado del sistema:');
  console.log(`   - Ollama disponible: ${stats.ollamaAvailable ? 'âœ…' : 'âŒ'}`);
  console.log(`   - Modelo: ${stats.model}`);
  console.log(`   - URL: ${stats.baseUrl}`);
  console.log(`   - Respuestas locales: ${stats.localResponsesCount}`);
  console.log('');

  if (!stats.ollamaAvailable) {
    console.log('âš ï¸  Ollama no estÃ¡ disponible. Solo se probarÃ¡n respuestas locales.');
    console.log('');
  }

  const customerPhone = '+573001234567';
  let totalTime = 0;
  let localCount = 0;
  let ollamaCount = 0;

  for (let i = 0; i < testCases.length; i++) {
    const test = testCases[i];
    
    console.log(`\n${'='.repeat(80)}`);
    console.log(`[${i + 1}/${testCases.length}] ${test.name}`);
    console.log(`${'='.repeat(80)}`);
    console.log(`ğŸ“ Mensaje: "${test.message}"`);
    console.log(`ğŸ’¡ DescripciÃ³n: ${test.description}`);
    console.log('');

    const startTime = Date.now();

    try {
      const response = await HybridBotService.processMessage(
        test.message,
        customerPhone
      );

      const duration = Date.now() - startTime;
      totalTime += duration;

      if (response.source === 'local') localCount++;
      else ollamaCount++;

      console.log(`\nâœ… Respuesta recibida en ${(duration / 1000).toFixed(2)}s`);
      console.log(`ğŸ“ Fuente: ${response.source.toUpperCase()}`);
      console.log(`ğŸ¯ IntenciÃ³n: ${response.intent || 'N/A'}`);
      console.log(`ğŸ“Š Confianza: ${(response.confidence * 100).toFixed(0)}%`);
      
      if (response.needsHumanEscalation) {
        console.log(`âš ï¸  Requiere escalamiento humano`);
      }

      console.log(`\nğŸ’¬ Respuesta del bot:\n`);
      console.log(response.message);

    } catch (error: any) {
      console.error(`\nâŒ Error: ${error.message}`);
    }

    // Pausa entre tests
    if (i < testCases.length - 1) {
      console.log(`\nâ¸ï¸  Pausa de 2 segundos...\n`);
      await new Promise(resolve => setTimeout(resolve, 2000));
    }
  }

  // Resumen final
  console.log(`\n\n${'='.repeat(80)}`);
  console.log('ğŸ“Š RESUMEN FINAL');
  console.log(`${'='.repeat(80)}\n`);

  console.log(`âœ… Tests completados: ${testCases.length}`);
  console.log(`âš¡ Respuestas locales: ${localCount} (instantÃ¡neas)`);
  console.log(`ğŸ§  Respuestas Ollama: ${ollamaCount} (inteligentes)`);
  console.log(`â±ï¸  Tiempo total: ${(totalTime / 1000).toFixed(2)}s`);
  console.log(`ğŸ“ˆ Tiempo promedio: ${(totalTime / testCases.length / 1000).toFixed(2)}s`);

  const localPercentage = (localCount / testCases.length * 100).toFixed(0);
  const ollamaPercentage = (ollamaCount / testCases.length * 100).toFixed(0);

  console.log(`\nğŸ’¡ DistribuciÃ³n:`);
  console.log(`   - ${localPercentage}% respondido por bot local (gratis, instantÃ¡neo)`);
  console.log(`   - ${ollamaPercentage}% respondido por Ollama (inteligente, contextual)`);

  console.log(`\nğŸ¯ VENTAJAS DEL SISTEMA HÃBRIDO:`);
  console.log(`   âœ… Respuestas instantÃ¡neas para consultas comunes`);
  console.log(`   âœ… Inteligencia artificial para consultas complejas`);
  console.log(`   âœ… Memoria y contexto conversacional`);
  console.log(`   âœ… Costo optimizado (solo usa IA cuando es necesario)`);
  console.log(`   âœ… Fallback automÃ¡tico si Ollama falla`);

  console.log(`\nğŸ“ RECOMENDACIONES:`);
  console.log(`   1. Agregar mÃ¡s respuestas locales para consultas frecuentes`);
  console.log(`   2. Ollama maneja bien las consultas complejas`);
  console.log(`   3. La memoria contextual funciona perfectamente`);
  console.log(`   4. Tiempo de respuesta aceptable (~23s para Ollama)`);

  console.log('');
}

// Ejecutar test
runTest().catch(console.error);

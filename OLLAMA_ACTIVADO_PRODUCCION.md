# ğŸ¤– OLLAMA ACTIVADO EN PRODUCCIÃ“N

## âœ… ConfiguraciÃ³n Actual

**Ollama es ahora tu IA PRINCIPAL** para responder a clientes en WhatsApp.

### Orden de Prioridad de IAs:

```
1ï¸âƒ£ OLLAMA (Principal)
   â”œâ”€â”€ URL: https://bot-whatsapp-ollama.sqaoeo.easypanel.host
   â”œâ”€â”€ Modelo: gemma:2b
   â”œâ”€â”€ Velocidad: 3-15s (despuÃ©s de calentar)
   â”œâ”€â”€ Costo: $0 (GRATIS E ILIMITADO)
   â””â”€â”€ LÃ­mites: Ninguno âœ…

2ï¸âƒ£ GROQ (Fallback automÃ¡tico)
   â”œâ”€â”€ 8 API keys con rotaciÃ³n automÃ¡tica
   â”œâ”€â”€ Velocidad: 1-2s
   â”œâ”€â”€ Costo: Gratis hasta lÃ­mite
   â””â”€â”€ Se activa si Ollama falla

3ï¸âƒ£ BASE DE CONOCIMIENTO LOCAL (Ãšltimo recurso)
   â”œâ”€â”€ 158+ respuestas guardadas
   â””â”€â”€ Respuestas instantÃ¡neas
```

## ğŸ“Š DesempeÃ±o de Ollama

### Velocidad Observada:
- **Primera respuesta**: 65s (carga inicial)
- **Segunda respuesta**: 20s (calentamiento)
- **Tercera respuesta**: 9s (optimizando)
- **Cuarta respuesta**: 3s âš¡ (Ã³ptimo)
- **Promedio estable**: 3-15s

### Ventajas:
âœ… **Gratis e ilimitado** - Sin rate limits ni costos
âœ… **Privacidad total** - Todo en tu servidor
âœ… **Mejora con el uso** - Se optimiza automÃ¡ticamente
âœ… **Sin dependencias** - No depende de APIs externas
âœ… **Entrenamiento ilimitado** - Aprende sin lÃ­mites

### Desventajas:
âš ï¸ **Primera respuesta lenta** - 65s en arranque frÃ­o
âš ï¸ **MÃ¡s lento que Groq** - 3-15s vs 1-2s
âš ï¸ **Requiere servidor** - Necesita Easypanel corriendo

## ğŸš€ CÃ³mo Funciona

### Flujo de Respuesta:

```
Cliente envÃ­a mensaje
        â†“
Â¿Ollama disponible?
        â†“
    SÃ â†’ Ollama genera respuesta (3-15s)
        â†“
    âœ… Respuesta enviada
        â†“
    ğŸ’¾ Guardada en base de conocimiento
```

### Si Ollama Falla:

```
Ollama no responde
        â†“
Groq toma el control (1-2s)
        â†“
âœ… Respuesta enviada
        â†“
ğŸ’¾ Guardada en base de conocimiento
```

## ğŸ”§ Variables de Entorno Activas

```env
# Ollama como principal
OLLAMA_BASE_URL=https://bot-whatsapp-ollama.sqaoeo.easypanel.host
OLLAMA_MODEL=gemma:2b
OLLAMA_ENABLED=true
OLLAMA_TIMEOUT=60000
OLLAMA_MAX_TOKENS=500

# Sistema de IA
AI_PROVIDER=ollama
DEFAULT_AI_PROVIDER=ollama
AI_FALLBACK_ENABLED=true

# Groq como fallback
GROQ_API_KEY=YOUR_GROQ_API_KEY_HERE
# + 7 API keys adicionales para rotaciÃ³n
```

## ğŸ“ Comandos Ãštiles

### Verificar Ollama:
```bash
npx tsx scripts/verificar-ollama.ts
```

### Entrenar con Ollama:
```bash
npx tsx scripts/entrenar-solo-ollama.ts
```

### Entrenamiento automÃ¡tico completo:
```bash
npx tsx scripts/entrenar-bot-automatico.ts
```

### Probar URLs de Ollama:
```bash
npx tsx scripts/test-ollama-urls.ts
```

### Test simple de Ollama:
```bash
npx tsx scripts/test-ollama-simple.ts
```

## ğŸ¯ Recomendaciones

### Para ProducciÃ³n:
1. **Deja que Ollama se caliente** - Las primeras respuestas son lentas
2. **Monitorea el desempeÃ±o** - Observa los tiempos de respuesta
3. **Groq como respaldo** - EstÃ¡ configurado automÃ¡ticamente
4. **Entrena regularmente** - Mejora las respuestas con el tiempo

### Si Ollama es Muy Lento:
```bash
# Cambiar a Groq como principal
AI_PROVIDER=groq
DEFAULT_AI_PROVIDER=groq

# Mantener Ollama para entrenamiento
OLLAMA_ENABLED=true
```

### Si Quieres MÃ¡s Velocidad:
Considera usar un modelo mÃ¡s pequeÃ±o o aumentar recursos del servidor Ollama en Easypanel.

## ğŸ”„ Volver a Groq

Si decides que Ollama es muy lento para producciÃ³n:

```env
# En .env
AI_PROVIDER=groq
DEFAULT_AI_PROVIDER=groq
AI_FALLBACK_ENABLED=true

# Mantener Ollama para entrenamiento
OLLAMA_ENABLED=true
```

Luego reinicia el bot:
```bash
npm run dev
```

## ğŸ“ˆ Monitoreo

### Logs a Observar:
```
[IntelligentEngine] ğŸ¤– Intentando con Ollama local...
[Ollama] ğŸ¤– Generando respuesta con gemma:2b
[Ollama] âœ… Respuesta generada: ...
[IntelligentEngine] âœ… Respuesta generada con Ollama
```

### Si Ollama Falla:
```
[IntelligentEngine] âš ï¸ Ollama no disponible, usando Groq...
```

## âœ… Estado Actual

- âœ… Ollama conectado y funcionando
- âœ… Modelo gemma:2b disponible
- âœ… Configurado como IA principal
- âœ… Groq configurado como fallback
- âœ… 8 API keys de Groq con rotaciÃ³n
- âœ… Base de conocimiento con 158+ entradas
- âœ… Sistema de entrenamiento automÃ¡tico listo

## ğŸš€ Siguiente Paso

**Inicia el bot y prueba con clientes reales:**

```bash
npm run dev
```

Observa los logs para ver:
- Tiempos de respuesta de Ollama
- Calidad de las respuestas
- CuÃ¡ndo se activa el fallback a Groq

**Â¡Ollama estÃ¡ listo para producciÃ³n!** ğŸ‰

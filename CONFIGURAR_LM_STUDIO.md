# ğŸ® CÃ³mo Configurar LM Studio (Paso a Paso)

## ğŸ“¥ Paso 1: Descargar e Instalar

1. Ve a: **https://lmstudio.ai/**
2. Descarga la versiÃ³n para Windows
3. Instala el programa (siguiente, siguiente, instalar)
4. Abre LM Studio

## ğŸ“¦ Paso 2: Descargar un Modelo

1. En LM Studio, ve a la pestaÃ±a **"Search"** (ğŸ”)
2. Busca: **"phi-2"**
3. Encuentra: **"microsoft/phi-2"**
4. Haz clic en **"Download"**
5. Espera a que descargue (puede tardar unos minutos)

### Modelos Recomendados

| Modelo | TamaÃ±o | Velocidad | Calidad | Recomendado Para |
|--------|--------|-----------|---------|------------------|
| **phi-2** | 2.7GB | âš¡âš¡âš¡ | â­â­â­ | **Respuestas rÃ¡pidas** |
| llama-3.2-3b | 3GB | âš¡âš¡ | â­â­â­â­ | Conversaciones |
| mistral-7b | 7GB | âš¡ | â­â­â­â­â­ | MÃ¡xima calidad |

**RecomendaciÃ³n**: Empieza con **phi-2** (rÃ¡pido y ligero)

## âš™ï¸ Paso 3: Activar el Servidor API

1. Ve a **Settings** (âš™ï¸) en la esquina superior derecha
2. Busca la secciÃ³n **"Server"**
3. Activa: **"Enable local REST API server"**
4. Verifica que el puerto sea: **1234**
5. Verifica que la URL sea: **http://localhost:1234**

### ConfiguraciÃ³n Recomendada

```
âœ… Enable local REST API server: ON
âœ… Port: 1234
âœ… CORS: Enabled
âœ… Auto-start server: ON (opcional)
```

## ğŸš€ Paso 4: Cargar el Modelo

1. Ve a la pestaÃ±a **"Chat"** (ğŸ’¬)
2. En la parte superior, haz clic en **"Select a model"**
3. Elige el modelo que descargaste (phi-2)
4. Espera a que cargue (verÃ¡s una barra de progreso)
5. Cuando estÃ© listo, verÃ¡s: **"Model loaded"**

## âœ… Paso 5: Verificar que Funciona

### OpciÃ³n 1: Probar en LM Studio

1. En la pestaÃ±a **"Chat"**, escribe un mensaje
2. Ejemplo: "Hola, Â¿cÃ³mo estÃ¡s?"
3. Presiona Enter
4. DeberÃ­as recibir una respuesta

### OpciÃ³n 2: Probar desde tu Bot

```bash
# Ejecuta el script de prueba
probar-multi-provider.bat
```

DeberÃ­as ver:
```
âœ… LMSTUDIO: FUNCIONANDO
```

## ğŸ”§ ConfiguraciÃ³n Avanzada (Opcional)

### Ajustar Rendimiento

En **Settings** â†’ **Performance**:

```
GPU Acceleration: ON (si tienes GPU)
CPU Threads: Auto (o ajusta segÃºn tu PC)
Context Length: 2048 (suficiente para chat)
```

### Ajustar Calidad de Respuestas

En la pestaÃ±a **"Chat"** â†’ **Settings**:

```
Temperature: 0.7 (creatividad moderada)
Top P: 0.9 (diversidad de respuestas)
Max Tokens: 500 (longitud de respuesta)
```

## ğŸ¯ VerificaciÃ³n Final

### Checklist

- [ ] LM Studio instalado y abierto
- [ ] Modelo phi-2 descargado
- [ ] Servidor API activado (puerto 1234)
- [ ] Modelo cargado en Chat
- [ ] Probado con mensaje de prueba
- [ ] Script `probar-multi-provider.bat` muestra âœ…

## ğŸ” SoluciÃ³n de Problemas

### Problema: "Model not loaded"

**SoluciÃ³n**:
1. Ve a la pestaÃ±a **"Chat"**
2. Haz clic en **"Select a model"**
3. Elige phi-2
4. Espera a que cargue completamente

### Problema: "Server not running"

**SoluciÃ³n**:
1. Ve a **Settings** (âš™ï¸)
2. Desactiva y vuelve a activar **"Enable local REST API server"**
3. Verifica que el puerto sea 1234
4. Reinicia LM Studio si es necesario

### Problema: "Connection refused"

**SoluciÃ³n**:
1. Verifica que LM Studio estÃ© abierto
2. Verifica que el servidor estÃ© activado
3. Verifica que el modelo estÃ© cargado
4. Prueba acceder a: http://localhost:1234/v1/models
   - DeberÃ­as ver una lista de modelos

### Problema: "Respuestas muy lentas"

**SoluciÃ³n**:
1. Usa un modelo mÃ¡s pequeÃ±o (phi-2)
2. Activa GPU Acceleration en Settings
3. Reduce Context Length a 1024
4. Cierra otros programas pesados

## ğŸ’¡ Tips y Trucos

### Mantener LM Studio EjecutÃ¡ndose

Para que tu bot siempre tenga respaldo:

1. Configura LM Studio para iniciar con Windows
2. Activa **"Auto-start server"** en Settings
3. Minimiza LM Studio a la bandeja del sistema

### Usar MÃºltiples Modelos

Puedes tener varios modelos descargados:

1. Descarga phi-2 (rÃ¡pido)
2. Descarga llama-3.2-3b (mejor calidad)
3. Cambia entre ellos segÃºn necesites

En tu `.env`:
```env
LM_STUDIO_MODEL=phi-2
# O
LM_STUDIO_MODEL=llama-3.2-3b
```

### Monitorear Uso

En LM Studio puedes ver:
- Tokens por segundo
- Uso de memoria
- Tiempo de respuesta

Esto te ayuda a optimizar el rendimiento.

## ğŸ‰ Â¡Listo!

Ahora tienes LM Studio configurado y funcionando.

Tu bot puede usar esta IA local:
- âœ… Sin lÃ­mites de uso
- âœ… Sin costo
- âœ… Sin necesidad de internet
- âœ… Respuestas rÃ¡pidas

## ğŸš€ Siguiente Paso

Ejecuta el script de prueba:

```bash
probar-multi-provider.bat
```

DeberÃ­as ver:
```
âœ… GROQ: FUNCIONANDO
âœ… LMSTUDIO: FUNCIONANDO

ğŸ‰ Tu bot tiene respaldo automÃ¡tico!
```

---

**Â¿Necesitas ayuda?**

1. Revisa la secciÃ³n de SoluciÃ³n de Problemas
2. Verifica que LM Studio estÃ© ejecutÃ¡ndose
3. Prueba con el script `probar-multi-provider.bat`
4. Lee `GUIA_MULTI_PROVIDER_IA.md` para mÃ¡s detalles

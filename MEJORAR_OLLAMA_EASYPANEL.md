# üöÄ MEJORAR OLLAMA EN EASYPANEL

## Configuraci√≥n Actual

```env
OLLAMA_BASE_URL=https://bot-whatsapp-ollama.sqaoeo.easypanel.host
OLLAMA_MODEL=gemma:2b
OLLAMA_ENABLED=true
OLLAMA_TIMEOUT=10000
OLLAMA_MAX_TOKENS=300
AI_PROVIDER=ollama
```

## ‚ö†Ô∏è Problemas con `gemma:2b`

El modelo `gemma:2b` es MUY peque√±o:
- Solo 2 billones de par√°metros
- Respuestas de baja calidad
- Puede dar respuestas incoherentes
- No entiende bien el contexto

## ‚úÖ Modelos Recomendados

### Opci√≥n 1: `llama3.2:3b` (Recomendado)
```env
OLLAMA_MODEL=llama3.2:3b
OLLAMA_MAX_TOKENS=500
```

**Ventajas:**
- ‚úÖ 3 billones de par√°metros (50% m√°s potente)
- ‚úÖ Mejor comprensi√≥n del espa√±ol
- ‚úÖ Respuestas m√°s coherentes
- ‚úÖ R√°pido (2-3 segundos)
- ‚úÖ Usa ~2GB RAM

**Instalar:**
```bash
# Conectar a Easypanel Console del contenedor Ollama
docker exec -it ollama ollama pull llama3.2:3b
```

### Opci√≥n 2: `llama3.1:8b` (Mejor Calidad)
```env
OLLAMA_MODEL=llama3.1:8b
OLLAMA_MAX_TOKENS=800
```

**Ventajas:**
- ‚úÖ 8 billones de par√°metros (4x m√°s potente)
- ‚úÖ Excelente comprensi√≥n del espa√±ol
- ‚úÖ Respuestas de alta calidad
- ‚úÖ Mejor razonamiento
- ‚úÖ Usa ~4GB RAM

**Desventajas:**
- ‚ö†Ô∏è M√°s lento (4-6 segundos)
- ‚ö†Ô∏è Requiere m√°s RAM

**Instalar:**
```bash
docker exec -it ollama ollama pull llama3.1:8b
```

### Opci√≥n 3: `qwen2.5:3b` (Alternativa)
```env
OLLAMA_MODEL=qwen2.5:3b
OLLAMA_MAX_TOKENS=500
```

**Ventajas:**
- ‚úÖ Muy r√°pido
- ‚úÖ Bueno con productos y comercio
- ‚úÖ Usa ~2GB RAM

**Instalar:**
```bash
docker exec -it ollama ollama pull qwen2.5:3b
```

## üìä Comparaci√≥n

| Modelo | Par√°metros | RAM | Velocidad | Calidad | Recomendado |
|--------|-----------|-----|-----------|---------|-------------|
| gemma:2b | 2B | 1.5GB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | ‚ùå No |
| llama3.2:3b | 3B | 2GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ S√≠ |
| qwen2.5:3b | 3B | 2GB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | ‚úÖ S√≠ |
| llama3.1:8b | 8B | 4GB | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ S√≠ (si tienes RAM) |

## üîß C√≥mo Cambiar el Modelo

### Paso 1: Instalar el Modelo

Conecta a la consola de Easypanel del contenedor Ollama:

```bash
# Ver modelos instalados
docker exec -it ollama ollama list

# Instalar nuevo modelo (ejemplo: llama3.2:3b)
docker exec -it ollama ollama pull llama3.2:3b

# Verificar que se instal√≥
docker exec -it ollama ollama list
```

### Paso 2: Actualizar Variables de Entorno

En Easypanel ‚Üí Settings ‚Üí Environment Variables:

```env
OLLAMA_MODEL=llama3.2:3b
OLLAMA_MAX_TOKENS=500
OLLAMA_TIMEOUT=15000
```

### Paso 3: Reiniciar el Servicio

Reinicia el servicio del bot en Easypanel para que tome los cambios.

## üí° Configuraci√≥n √ìptima Recomendada

### Para Servidor con 4GB+ RAM:
```env
OLLAMA_BASE_URL=https://bot-whatsapp-ollama.sqaoeo.easypanel.host
OLLAMA_MODEL=llama3.1:8b
OLLAMA_ENABLED=true
OLLAMA_TIMEOUT=15000
OLLAMA_MAX_TOKENS=800
AI_PROVIDER=ollama
DEFAULT_AI_PROVIDER=ollama
AI_FALLBACK_ORDER=ollama,groq
```

### Para Servidor con 2-4GB RAM:
```env
OLLAMA_BASE_URL=https://bot-whatsapp-ollama.sqaoeo.easypanel.host
OLLAMA_MODEL=llama3.2:3b
OLLAMA_ENABLED=true
OLLAMA_TIMEOUT=12000
OLLAMA_MAX_TOKENS=500
AI_PROVIDER=ollama
DEFAULT_AI_PROVIDER=ollama
AI_FALLBACK_ORDER=ollama,groq
```

### Para Servidor con <2GB RAM:
```env
# Mantener gemma:2b pero aumentar tokens
OLLAMA_BASE_URL=https://bot-whatsapp-ollama.sqaoeo.easypanel.host
OLLAMA_MODEL=gemma:2b
OLLAMA_ENABLED=true
OLLAMA_TIMEOUT=10000
OLLAMA_MAX_TOKENS=500
AI_PROVIDER=ollama
DEFAULT_AI_PROVIDER=ollama
AI_FALLBACK_ORDER=ollama,groq
```

## üéØ Aumentar MAX_TOKENS

El valor actual de `300` es muy bajo. Recomendaciones:

- **M√≠nimo:** 500 tokens (respuestas completas)
- **Recomendado:** 800 tokens (respuestas detalladas)
- **M√°ximo:** 1000 tokens (conversaciones largas)

```env
# Cambiar de:
OLLAMA_MAX_TOKENS=300

# A:
OLLAMA_MAX_TOKENS=500  # O 800 si tienes recursos
```

## üîç Verificar que Funciona

### 1. Ver logs del bot:
```
[Ollama] Usando modelo llama3.2:3b
[Ollama] ‚úÖ Respuesta generada en 2.3s
```

### 2. Probar con mensaje:
Env√≠a: "Hola, tienes laptops para dise√±o gr√°fico?"

**Con gemma:2b (malo):**
```
S√≠ tengo laptop
```

**Con llama3.2:3b (bueno):**
```
¬°Claro! Tengo varias opciones de laptops ideales para dise√±o gr√°fico:

1Ô∏è‚É£ Laptop HP con Ryzen 7
   üí∞ $2.500.000
   
2Ô∏è‚É£ Laptop Asus con Intel i7
   üí∞ $2.800.000

¬øCu√°l te interesa?
```

## üö® Troubleshooting

### Ollama muy lento

**Soluci√≥n 1:** Usar modelo m√°s peque√±o
```env
OLLAMA_MODEL=llama3.2:3b
```

**Soluci√≥n 2:** Aumentar recursos en Easypanel
- CPU: 2+ cores
- RAM: 4GB+

### Respuestas de baja calidad

**Soluci√≥n:** Cambiar a modelo m√°s grande
```env
OLLAMA_MODEL=llama3.1:8b
```

### Error "model not found"

**Soluci√≥n:** Instalar el modelo
```bash
docker exec -it ollama ollama pull llama3.2:3b
```

## üìà Resultado Esperado

Con `llama3.2:3b` o superior:
- ‚úÖ Respuestas coherentes y naturales
- ‚úÖ Mejor comprensi√≥n del espa√±ol
- ‚úÖ Respuestas m√°s completas
- ‚úÖ Mejor manejo de productos
- ‚úÖ Conversaciones m√°s fluidas

---

**Recomendaci√≥n Final:** Cambia a `llama3.2:3b` como m√≠nimo para tener respuestas de calidad aceptable.

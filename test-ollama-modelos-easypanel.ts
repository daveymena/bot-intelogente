/**
 * Test de Modelos Ollama en Easypanel
 * Prueba llama3 y mistral con preguntas sobre productos
 */

import Anthropic from '@anthropic-ai/sdk';

const OLLAMA_BASE_URL = 'https://davey-ollama.mapf5v.easypanel.host';

interface OllamaResponse {
  model: string;
  created_at: string;
  message: {
    role: string;
    content: string;
  };
  done: boolean;
  total_duration?: number;
  load_duration?: number;
  prompt_eval_duration?: number;
  eval_duration?: number;
}

async function testOllamaModel(model: string, prompt: string): Promise<void> {
  console.log(`\n${'='.repeat(80)}`);
  console.log(`ğŸ¤– Probando modelo: ${model}`);
  console.log(`${'='.repeat(80)}`);
  console.log(`ğŸ“ Pregunta: ${prompt}`);
  console.log(`â±ï¸  Iniciando...`);

  const startTime = Date.now();

  try {
    const response = await fetch(`${OLLAMA_BASE_URL}/api/chat`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: model,
        messages: [
          {
            role: 'system',
            content: `Eres un asistente de ventas experto en productos tecnolÃ³gicos. 
Respondes en espaÃ±ol de forma clara, concisa y profesional.
Tienes conocimiento sobre laptops, computadores, cursos digitales y megapacks.`
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        stream: false,
        options: {
          temperature: 0.7,
          num_predict: 500
        }
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`);
    }

    const data: OllamaResponse = await response.json();
    const endTime = Date.now();
    const duration = ((endTime - startTime) / 1000).toFixed(2);

    console.log(`\nâœ… Respuesta recibida en ${duration}s`);
    console.log(`\nğŸ’¬ Respuesta del modelo:\n`);
    console.log(data.message.content);

    // MÃ©tricas de rendimiento
    if (data.total_duration) {
      console.log(`\nğŸ“Š MÃ©tricas de rendimiento:`);
      console.log(`   - DuraciÃ³n total: ${(data.total_duration / 1e9).toFixed(2)}s`);
      if (data.load_duration) {
        console.log(`   - Carga del modelo: ${(data.load_duration / 1e9).toFixed(2)}s`);
      }
      if (data.prompt_eval_duration) {
        console.log(`   - EvaluaciÃ³n del prompt: ${(data.prompt_eval_duration / 1e9).toFixed(2)}s`);
      }
      if (data.eval_duration) {
        console.log(`   - GeneraciÃ³n: ${(data.eval_duration / 1e9).toFixed(2)}s`);
      }
    }

  } catch (error) {
    console.error(`\nâŒ Error al probar ${model}:`, error);
  }
}

async function runTests() {
  console.log(`
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   TEST DE MODELOS OLLAMA EN EASYPANEL                      â•‘
â•‘                                                                            â•‘
â•‘  Servidor: ${OLLAMA_BASE_URL}                    â•‘
â•‘  Modelos: llama3:latest, mistral:latest                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  `);

  // Preguntas de prueba sobre productos
  const testQuestions = [
    {
      question: "Â¿QuÃ© laptop me recomiendas para diseÃ±o grÃ¡fico?",
      description: "Consulta sobre producto especÃ­fico"
    },
    {
      question: "Necesito un computador econÃ³mico para estudiar, Â¿quÃ© opciones tienes?",
      description: "BÃºsqueda por presupuesto"
    },
    {
      question: "Â¿CuÃ¡l es la diferencia entre un curso de piano y un megapack de mÃºsica?",
      description: "ComparaciÃ³n de productos digitales"
    },
    {
      question: "Busco una moto para ciudad, Â¿quÃ© me recomiendas?",
      description: "Producto fÃ­sico especÃ­fico"
    }
  ];

  const models = ['llama3:latest', 'mistral:latest'];

  for (const model of models) {
    console.log(`\n\n${'â–ˆ'.repeat(80)}`);
    console.log(`â–ˆ  MODELO: ${model.toUpperCase().padEnd(68)}â–ˆ`);
    console.log(`${'â–ˆ'.repeat(80)}\n`);

    for (let i = 0; i < testQuestions.length; i++) {
      const test = testQuestions[i];
      console.log(`\n[${i + 1}/${testQuestions.length}] ${test.description}`);
      await testOllamaModel(model, test.question);
      
      // Pausa entre preguntas
      if (i < testQuestions.length - 1) {
        await new Promise(resolve => setTimeout(resolve, 2000));
      }
    }

    // Pausa entre modelos
    if (model !== models[models.length - 1]) {
      console.log(`\n\nâ¸ï¸  Pausa de 5 segundos antes del siguiente modelo...\n`);
      await new Promise(resolve => setTimeout(resolve, 5000));
    }
  }

  console.log(`\n\n${'='.repeat(80)}`);
  console.log(`âœ… TESTS COMPLETADOS`);
  console.log(`${'='.repeat(80)}\n`);
  
  console.log(`ğŸ“Š RESUMEN:`);
  console.log(`   - Modelos probados: ${models.length}`);
  console.log(`   - Preguntas por modelo: ${testQuestions.length}`);
  console.log(`   - Total de tests: ${models.length * testQuestions.length}`);
  console.log(`\nğŸ’¡ RECOMENDACIÃ“N:`);
  console.log(`   Revisa las respuestas y tiempos de cada modelo para decidir cuÃ¡l usar.`);
  console.log(`   - llama3: Generalmente mÃ¡s preciso y coherente`);
  console.log(`   - mistral: Puede ser mÃ¡s rÃ¡pido en algunas consultas\n`);
}

// Ejecutar tests
runTests().catch(console.error);

# ğŸ¤– Sistema Multi-Provider de IA con Fallback AutomÃ¡tico

## ğŸ¯ Â¿QuÃ© es esto?

Un sistema inteligente que usa **mÃºltiples APIs de IA** para que tu bot **nunca se quede sin respuestas**. Si una API falla, automÃ¡ticamente usa las otras.

## âœ¨ CaracterÃ­sticas

- âœ… **Fallback automÃ¡tico**: Si Groq falla â†’ usa LM Studio â†’ usa OpenAI
- âœ… **Sin lÃ­mites locales**: LM Studio corre en tu PC sin costos
- âœ… **ConfiguraciÃ³n flexible**: Elige el orden de prioridad
- âœ… **DetecciÃ³n de errores**: Cambia de API en milisegundos
- âœ… **FÃ¡cil de usar**: Solo activa y funciona

## ğŸ”§ ConfiguraciÃ³n

### 1. Instalar LM Studio

1. Descarga LM Studio: https://lmstudio.ai/
2. Instala y abre LM Studio
3. Descarga el modelo `phi-2` (o cualquier otro)
4. Ve a **Settings** â†’ Activa **"Enable local REST API server"**
5. AsegÃºrate que estÃ© en el puerto `1234`

### 2. Configurar Variables de Entorno

Tu archivo `.env` ya estÃ¡ configurado con:

```env
# Sistema de Fallback Multi-API
AI_PROVIDER=multi
AI_FALLBACK_ENABLED=true
AI_FALLBACK_ORDER=groq,lmstudio,openai

# Groq (Principal - RÃ¡pido y gratis)
GROQ_API_KEY=tu_api_key_aqui
GROQ_MODEL=llama-3.1-8b-instant

# LM Studio (Local - Sin lÃ­mites)
LM_STUDIO_URL=http://localhost:1234/v1/chat/completions
LM_STUDIO_MODEL=phi-2

# OpenAI (Opcional - Premium)
# OPENAI_API_KEY=tu_openai_key_aqui
# OPENAI_MODEL=gpt-3.5-turbo
```

### 3. Orden de Fallback

Puedes cambiar el orden en `AI_FALLBACK_ORDER`:

```env
# Orden recomendado (rÃ¡pido â†’ local â†’ premium)
AI_FALLBACK_ORDER=groq,lmstudio,openai

# Si prefieres usar LM Studio primero (sin internet)
AI_FALLBACK_ORDER=lmstudio,groq,openai

# Solo Groq y LM Studio (sin OpenAI)
AI_FALLBACK_ORDER=groq,lmstudio
```

## ğŸš€ CÃ³mo Funciona

### Flujo AutomÃ¡tico

```
Cliente envÃ­a mensaje
    â†“
Intenta con Groq
    â†“
Â¿FuncionÃ³? â†’ SÃ â†’ Responde
    â†“ NO
Intenta con LM Studio
    â†“
Â¿FuncionÃ³? â†’ SÃ â†’ Responde
    â†“ NO
Intenta con OpenAI
    â†“
Â¿FuncionÃ³? â†’ SÃ â†’ Responde
    â†“ NO
Error: Todas las APIs fallaron
```

### Ejemplo Real

```typescript
// El bot automÃ¡ticamente hace esto:

// 1. Intenta con Groq
[AI Multi-Provider] ğŸ”„ Intentando con: groq
[AI Multi-Provider] âœ… Ã‰xito con: groq
// Responde al cliente

// Si Groq falla:
[AI Multi-Provider] ğŸ”„ Intentando con: groq
[AI Multi-Provider] âŒ Error con groq: timeout
[AI Multi-Provider] ğŸ”„ Intentando con: lmstudio
[AI Multi-Provider] âœ… Ã‰xito con: lmstudio
// Responde al cliente con LM Studio
```

## ğŸ§ª Probar el Sistema

### OpciÃ³n 1: Script AutomÃ¡tico

```bash
# Ejecuta el archivo .bat
probar-multi-provider.bat
```

### OpciÃ³n 2: Comando Manual

```bash
npx tsx scripts/test-multi-provider.ts
```

### QuÃ© VerÃ¡s

```
ğŸ§ª Probando Sistema Multi-Provider de IA
============================================================

ğŸ“¡ Paso 1: Probando conectividad de todos los providers...

âœ… GROQ: FUNCIONANDO
âœ… LMSTUDIO: FUNCIONANDO
âŒ OPENAI: NO DISPONIBLE

ğŸ¤– Paso 2: Probando generaciÃ³n de respuesta con fallback...

âœ… RESPUESTA RECIBIDA:
Provider usado: GROQ
Modelo: llama-3.1-8b-instant

Contenido:
Â¡Hola! SÃ­, tenemos laptops disponibles...
```

## ğŸ“Š ComparaciÃ³n de Providers

| Provider | Velocidad | Costo | LÃ­mites | Requiere Internet |
|----------|-----------|-------|---------|-------------------|
| **Groq** | âš¡âš¡âš¡ Ultra rÃ¡pido | ğŸ†“ Gratis | âœ… Generosos | âœ… SÃ­ |
| **LM Studio** | âš¡âš¡ RÃ¡pido | ğŸ†“ Gratis | â™¾ï¸ Sin lÃ­mites | âŒ No |
| **OpenAI** | âš¡ Normal | ğŸ’° Pago | âš ï¸ Por uso | âœ… SÃ­ |

## ğŸ’¡ Recomendaciones

### Para MÃ¡xima Confiabilidad

```env
AI_FALLBACK_ORDER=groq,lmstudio,openai
```

- Groq es ultra rÃ¡pido (principal)
- LM Studio como respaldo local
- OpenAI como Ãºltimo recurso

### Para Trabajar Sin Internet

```env
AI_FALLBACK_ORDER=lmstudio,groq
```

- LM Studio primero (local)
- Groq como respaldo (si hay internet)

### Para MÃ¡xima Calidad

```env
AI_FALLBACK_ORDER=openai,groq,lmstudio
```

- OpenAI primero (mejor calidad)
- Groq como respaldo rÃ¡pido
- LM Studio como Ãºltimo recurso

## ğŸ” SoluciÃ³n de Problemas

### LM Studio no funciona

**Error**: `LM Studio fallÃ³: fetch failed`

**SoluciÃ³n**:
1. Abre LM Studio
2. Ve a **Settings**
3. Activa **"Enable local REST API server"**
4. Verifica que estÃ© en `http://localhost:1234`
5. Carga un modelo (phi-2 recomendado)

### Groq da timeout

**Error**: `Groq timeout`

**SoluciÃ³n**:
1. Verifica tu `GROQ_API_KEY` en `.env`
2. Aumenta el timeout:
   ```env
   GROQ_TIMEOUT=15000
   ```
3. Prueba con otro modelo:
   ```env
   GROQ_MODEL=llama-3.3-70b-versatile
   ```

### Todas las APIs fallan

**Error**: `Todas las APIs de IA fallaron`

**SoluciÃ³n**:
1. Verifica tu conexiÃ³n a internet
2. AsegÃºrate que LM Studio estÃ© ejecutÃ¡ndose
3. Verifica tus API keys en `.env`
4. Ejecuta el script de prueba:
   ```bash
   probar-multi-provider.bat
   ```

## ğŸ® Uso en el Bot

El sistema ya estÃ¡ integrado automÃ¡ticamente. Solo necesitas:

1. **Activar el sistema**:
   ```env
   AI_FALLBACK_ENABLED=true
   ```

2. **Iniciar tu bot**:
   ```bash
   npm run dev
   ```

3. **Â¡Listo!** El bot usarÃ¡ automÃ¡ticamente el sistema multi-provider

## ğŸ“ˆ Monitoreo

El sistema registra en consola quÃ© provider estÃ¡ usando:

```
[AI Multi-Provider] ğŸ”„ Usando sistema multi-provider con fallback automÃ¡tico
[AI Multi-Provider] ğŸ”„ Intentando con: groq
[AI Multi-Provider] âœ… Ã‰xito con: groq
[AI] âœ… Respuesta generada con: groq (llama-3.1-8b-instant)
```

Si un provider falla:

```
[AI Multi-Provider] ğŸ”„ Intentando con: groq
[AI Multi-Provider] âŒ Error con groq: timeout
[AI Multi-Provider] ğŸ”„ Intentando con: lmstudio
[AI Multi-Provider] âœ… Ã‰xito con: lmstudio
[AI] âœ… Respuesta generada con: lmstudio (phi-2)
```

## ğŸ¯ Ventajas del Sistema

1. **Nunca se cae**: Si una API falla, usa otra automÃ¡ticamente
2. **Sin lÃ­mites**: LM Studio corre local sin restricciones
3. **RÃ¡pido**: Groq responde en milisegundos
4. **Flexible**: Configura el orden que prefieras
5. **Transparente**: Ves quÃ© provider estÃ¡ usando
6. **FÃ¡cil**: Solo activa y funciona

## ğŸš€ PrÃ³ximos Pasos

1. âœ… Ejecuta `probar-multi-provider.bat` para verificar
2. âœ… AsegÃºrate que LM Studio estÃ© ejecutÃ¡ndose
3. âœ… Inicia tu bot con `npm run dev`
4. âœ… EnvÃ­a mensajes de prueba por WhatsApp
5. âœ… Observa los logs para ver quÃ© provider usa

## ğŸ“ Soporte

Si tienes problemas:

1. Revisa los logs en consola
2. Ejecuta el script de prueba
3. Verifica tu configuraciÃ³n en `.env`
4. AsegÃºrate que LM Studio estÃ© ejecutÃ¡ndose

---

**Â¡Tu bot ahora tiene respaldo automÃ¡tico de IA! ğŸ‰**

Nunca mÃ¡s te quedarÃ¡s sin respuestas, incluso si una API falla.

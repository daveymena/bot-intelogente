// Test para verificar qu√© modelos tiene Ollama
require('dotenv').config();

async function checkOllamaModels() {
  console.log('üîç Verificando modelos disponibles en Ollama\n');
  console.log('üìç URL:', process.env.OLLAMA_BASE_URL);
  console.log('ü§ñ Modelo configurado:', process.env.OLLAMA_MODEL);
  console.log('');

  // Test 1: Verificar que Ollama responde
  console.log('1Ô∏è‚É£ Test: ¬øOllama est√° activo?');
  try {
    const response = await fetch(`${process.env.OLLAMA_BASE_URL}/api/tags`);
    
    if (response.ok) {
      const data = await response.json();
      console.log('  ‚úÖ Ollama est√° activo');
      console.log('  üì¶ Modelos disponibles:');
      
      if (data.models && data.models.length > 0) {
        data.models.forEach(model => {
          const isConfigured = model.name === process.env.OLLAMA_MODEL;
          console.log(`    ${isConfigured ? '‚úÖ' : '  '} ${model.name} (${(model.size / 1024 / 1024 / 1024).toFixed(2)} GB)`);
        });
      } else {
        console.log('    ‚ö†Ô∏è  No hay modelos instalados');
      }
    } else {
      console.log('  ‚ùå Error:', response.status, response.statusText);
    }
  } catch (error) {
    console.log('  ‚ùå Error conectando:', error.message);
  }

  console.log('');

  // Test 2: Probar el modelo configurado
  console.log('2Ô∏è‚É£ Test: ¬øEl modelo configurado funciona?');
  try {
    const response = await fetch(`${process.env.OLLAMA_BASE_URL}/api/generate`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: process.env.OLLAMA_MODEL,
        prompt: 'Di solo "OK"',
        stream: false
      })
    });

    if (response.ok) {
      const data = await response.json();
      console.log('  ‚úÖ Modelo funciona correctamente');
      console.log('  üìù Respuesta:', data.response);
    } else {
      console.log('  ‚ùå Error:', response.status, response.statusText);
      
      if (response.status === 404) {
        console.log('  üí° El modelo no existe. Modelos disponibles arriba ‚òùÔ∏è');
      }
    }
  } catch (error) {
    console.log('  ‚ùå Error:', error.message);
  }

  console.log('');

  // Test 3: Probar con llama3.2:3b (el anterior)
  console.log('3Ô∏è‚É£ Test: ¬øFunciona llama3.2:3b?');
  try {
    const response = await fetch(`${process.env.OLLAMA_BASE_URL}/api/generate`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: 'llama3.2:3b',
        prompt: 'Di solo "OK"',
        stream: false
      })
    });

    if (response.ok) {
      const data = await response.json();
      console.log('  ‚úÖ llama3.2:3b funciona');
      console.log('  üìù Respuesta:', data.response);
      console.log('  üí° Puedes usar este modelo en .env');
    } else {
      console.log('  ‚ùå Error:', response.status);
    }
  } catch (error) {
    console.log('  ‚ùå Error:', error.message);
  }

  console.log('');
  console.log('üìã RECOMENDACI√ìN:');
  console.log('  - Si llama3.2:1b no existe, usa llama3.2:3b');
  console.log('  - O instala llama3.2:1b en Ollama con: ollama pull llama3.2:1b');
  console.log('  - Actualiza .env con el modelo que funcione');
}

checkOllamaModels().catch(console.error);

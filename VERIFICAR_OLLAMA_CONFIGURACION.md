# üîç VERIFICAR CONFIGURACI√ìN DE OLLAMA

## ‚ö†Ô∏è PROBLEMAS DETECTADOS

### 1. Ollama devuelve 404
```
[Orchestrator] ‚ö†Ô∏è Ollama fall√≥: Ollama HTTP 404
```

**Causa:** El modelo `llama3.2:3b` no est√° instalado en el servidor Ollama.

### 2. Error de Prisma con tags
```
Unknown argument `has`. Did you mean `in`?
```

**Causa:** Sintaxis incorrecta de Prisma para arrays.  
**Soluci√≥n:** ‚úÖ Ya corregido (cambi√© `has` por `hasSome`)

---

## ‚úÖ SOLUCIONES APLICADAS

### 1. Configuraci√≥n Corregida en .env

```bash
# OLLAMA (Principal)
OLLAMA_URL=https://davey-ollama2.mapf5v.easypanel.host
OLLAMA_MODEL=llama3.2:3b
OLLAMA_MODEL_FAST=gemma2:2b
OLLAMA_TIMEOUT=45000  # 45 segundos (m√°s tiempo)
```

### 2. Error de Prisma Corregido

```typescript
// ANTES (‚ùå Error)
{ tags: { has: keyword } }

// AHORA (‚úÖ Correcto)
{ tags: { hasSome: [keyword] } }
```

### 3. Timeout Aumentado

- **Antes:** 20 segundos
- **Ahora:** 45 segundos (m√°s tiempo para Ollama)

---

## üöÄ PASOS PARA VERIFICAR

### Paso 1: Verificar Configuraci√≥n

```bash
VERIFICAR_OLLAMA_AHORA.bat
```

Este script hace:
1. ‚úÖ Verifica conexi√≥n a Ollama
2. ‚úÖ Lista modelos disponibles
3. ‚úÖ Verifica que `llama3.2:3b` est√© instalado
4. ‚úÖ Prueba generaci√≥n de texto

### Paso 2: Instalar Modelo (si no existe)

Si el modelo no est√° instalado, ejecutar en **Easypanel**:

```bash
# Conectar al contenedor de Ollama
docker exec -it <ollama-container> bash

# Instalar modelo
ollama pull llama3.2:3b

# Verificar instalaci√≥n
ollama list
```

### Paso 3: Probar Solo Ollama

```bash
npx tsx scripts/test-solo-ollama.ts
```

Esto prueba Ollama **sin fallbacks** para verificar que funciona correctamente.

---

## üîß MODELOS DISPONIBLES

### Recomendados para Producci√≥n

| Modelo | Tama√±o | Velocidad | Calidad | Uso |
|--------|--------|-----------|---------|-----|
| **llama3.2:3b** | 3B | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Principal |
| **gemma2:2b** | 2B | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | R√°pido |
| **mistral:7b** | 7B | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Calidad |

### Instalar Modelos

```bash
# Modelo principal (recomendado)
ollama pull llama3.2:3b

# Modelo r√°pido (alternativa)
ollama pull gemma2:2b

# Modelo de alta calidad (si tienes recursos)
ollama pull mistral:7b
```

---

## üìä VERIFICAR ESTADO ACTUAL

### 1. Verificar Conexi√≥n

```bash
curl https://davey-ollama2.mapf5v.easypanel.host/api/tags
```

**Respuesta esperada:**
```json
{
  "models": [
    {
      "name": "llama3.2:3b",
      "modified_at": "2024-11-26T...",
      "size": 2000000000
    }
  ]
}
```

### 2. Verificar Generaci√≥n

```bash
curl -X POST https://davey-ollama2.mapf5v.easypanel.host/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.2:3b",
    "prompt": "Responde en una palabra: ¬øCapital de Colombia?",
    "stream": false
  }'
```

**Respuesta esperada:**
```json
{
  "model": "llama3.2:3b",
  "response": "Bogot√°",
  "done": true
}
```

---

## üêõ TROUBLESHOOTING

### Problema: 404 Not Found

**Causa:** Modelo no instalado

**Soluci√≥n:**
```bash
ollama pull llama3.2:3b
```

### Problema: Timeout

**Causa:** Modelo muy lento o servidor sobrecargado

**Soluci√≥n:**
1. Usar modelo m√°s r√°pido: `gemma2:2b`
2. Aumentar timeout en c√≥digo
3. Verificar recursos del servidor

### Problema: Connection Refused

**Causa:** Ollama no est√° corriendo

**Soluci√≥n:**
1. Verificar que el contenedor est√© activo en Easypanel
2. Verificar la URL correcta
3. Verificar firewall/puertos

### Problema: Respuestas de Baja Calidad

**Causa:** Modelo muy peque√±o o temperatura incorrecta

**Soluci√≥n:**
1. Usar modelo m√°s grande: `mistral:7b`
2. Ajustar temperatura (0.5-0.9)
3. Mejorar el prompt

---

## üìù SCRIPTS CREADOS

### 1. `scripts/verificar-ollama-simple.ts`
Verifica configuraci√≥n b√°sica de Ollama

### 2. `scripts/test-solo-ollama.ts`
Prueba Ollama sin fallbacks

### 3. `VERIFICAR_OLLAMA_AHORA.bat`
Ejecuta ambos scripts autom√°ticamente

---

## ‚úÖ CHECKLIST DE VERIFICACI√ìN

- [ ] Ollama responde en `/api/tags`
- [ ] Modelo `llama3.2:3b` est√° instalado
- [ ] Genera texto correctamente
- [ ] Tiempo de respuesta < 10 segundos
- [ ] No hay errores de Prisma
- [ ] Variables de entorno correctas

---

## üéØ PR√ìXIMOS PASOS

### 1. Ejecutar Verificaci√≥n
```bash
VERIFICAR_OLLAMA_AHORA.bat
```

### 2. Si Todo Est√° OK
```bash
# Probar el orquestador completo
npx tsx scripts/test-ollama-orchestrator.ts
```

### 3. Si Hay Problemas
- Revisar logs de Easypanel
- Verificar que el modelo est√© instalado
- Probar con modelo alternativo (`gemma2:2b`)

---

## üìû COMANDOS √öTILES

### Ver Logs de Ollama (Easypanel)
```bash
docker logs <ollama-container> --tail 100 -f
```

### Listar Modelos Instalados
```bash
ollama list
```

### Eliminar Modelo (si necesitas espacio)
```bash
ollama rm <modelo>
```

### Ver Uso de Recursos
```bash
docker stats <ollama-container>
```

---

## üéâ RESULTADO ESPERADO

Despu√©s de la verificaci√≥n, deber√≠as ver:

```
‚úÖ Conexi√≥n exitosa!
‚úÖ Modelo "llama3.2:3b" est√° disponible
‚úÖ Respuesta generada en 3500ms
‚úÖ Respuesta correcta! Ollama est√° funcionando bien.

‚úÖ OLLAMA EST√Å CORRECTAMENTE CONFIGURADO
```

---

**Creado:** 26 Nov 2025  
**Estado:** üîß Pendiente de verificaci√≥n  
**Prioridad:** üî¥ Alta

# ğŸ¤– GuÃ­a Completa del Sistema LLM

## ğŸ“‹ Tabla de Contenidos

1. [IntroducciÃ³n](#introducciÃ³n)
2. [Arquitectura del Sistema](#arquitectura-del-sistema)
3. [ConfiguraciÃ³n](#configuraciÃ³n)
4. [Uso BÃ¡sico](#uso-bÃ¡sico)
5. [PersonalizaciÃ³n](#personalizaciÃ³n)
6. [OptimizaciÃ³n](#optimizaciÃ³n)
7. [Troubleshooting](#troubleshooting)
8. [Mejores PrÃ¡cticas](#mejores-prÃ¡cticas)

---

## ğŸ¯ IntroducciÃ³n

El sistema LLM (Large Language Model) del bot utiliza **Groq con Llama 3.1** para generar respuestas inteligentes y naturales a los clientes por WhatsApp.

### CaracterÃ­sticas Principales

- âœ… Respuestas en espaÃ±ol natural y conversacional
- âœ… Contexto de conversaciÃ³n de 24 horas
- âœ… BÃºsqueda inteligente de productos
- âœ… DetecciÃ³n automÃ¡tica de intenciones
- âœ… EnvÃ­o automÃ¡tico de fotos y links de pago
- âœ… Escalamiento a humano cuando es necesario
- âœ… Formato automÃ¡tico con emojis y viÃ±etas

---

## ğŸ—ï¸ Arquitectura del Sistema

### Flujo de Procesamiento

```
1. Cliente envÃ­a mensaje por WhatsApp
   â†“
2. Baileys Service recibe el mensaje
   â†“
3. Sistema de Prioridades:
   
   PRIORIDAD 1: Â¿Es respuesta directa?
   (Saludos, gracias, horarios)
   â†’ SÃ: Responder sin IA (< 100ms)
   â†’ NO: Continuar
   
   PRIORIDAD 2: Â¿Solicita foto o link de pago?
   â†’ SÃ: Enviar automÃ¡ticamente
   â†’ NO: Continuar
   
   PRIORIDAD 3: Usar IA (Groq)
   â†’ Cargar historial de 24h
   â†’ Generar respuesta inteligente
   â†’ Formatear con emojis
   â†’ Enviar al cliente
   
4. Post-procesamiento:
   â†’ Detectar productos mencionados
   â†’ Enviar fotos automÃ¡ticamente
   â†’ Actualizar contexto
```

### Componentes Principales

| Componente | Archivo | FunciÃ³n |
|------------|---------|---------|
| **AI Service** | `src/lib/ai-service.ts` | Orquestador principal de IA |
| **Baileys Service** | `src/lib/baileys-stable-service.ts` | IntegraciÃ³n WhatsApp |
| **Direct Response** | `src/lib/direct-response-handler.ts` | Respuestas sin IA |
| **Product Intelligence** | `src/lib/product-intelligence-service.ts` | BÃºsqueda de productos |
| **Response Formatter** | `src/lib/response-formatter.ts` | Formato de respuestas |
| **Context Service** | `src/lib/conversation-context-service.ts` | Memoria de conversaciÃ³n |

---

## âš™ï¸ ConfiguraciÃ³n

### 1. Variables de Entorno

Edita el archivo `.env`:

```env
# IA Principal
AI_PROVIDER=groq
GROQ_API_KEY=tu_api_key_aqui
GROQ_MODEL=llama-3.1-8b-instant
GROQ_MAX_TOKENS=300

# CaracterÃ­sticas
AI_ENABLED=true
PHOTOS_ENABLED=true
AUDIO_ENABLED=true
```

### 2. ConfiguraciÃ³n del LLM

Edita `llm-config.json`:

```json
{
  "groq": {
    "model": "llama-3.1-8b-instant",
    "maxTokens": 300,
    "temperature": 0.7
  },
  "systemPrompt": {
    "tone": "amigable y conversacional",
    "style": {
      "useEmojis": true,
      "maxLines": 4
    }
  }
}
```

### 3. Personalidad del Bot

Desde el dashboard:
1. Ve a **ConfiguraciÃ³n** â†’ **Personalidad del Bot**
2. Configura:
   - Nombre del bot
   - Tono de voz
   - Estilo de respuestas
   - Emojis favoritos

---

## ğŸš€ Uso BÃ¡sico

### Iniciar el Sistema

```bash
# Desarrollo
npm run dev

# ProducciÃ³n
npm start
```

### Probar el LLM

```bash
# Test completo
npm run test:llm
# o
test-llm.bat

# Test especÃ­fico
npx tsx scripts/test-llm-completo.ts
```

### Ver Logs en Tiempo Real

Los logs del LLM aparecen en la consola:

```
[AI] Generando respuesta para: "busco una laptop"
[AI] ğŸ“š Historial cargado: 4 mensajes
[AI] IntenciÃ³n detectada: product_search (0.95)
[AI] âœ… Respuesta generada con Groq (1.2s)
[Baileys] ğŸ¨ Respuesta formateada
[Baileys] âœ… Respuesta enviada
```

---

## ğŸ¨ PersonalizaciÃ³n

### 1. Modificar el System Prompt

Edita `src/lib/ai-service.ts`:

```typescript
const systemPrompt = `
Eres ${botName}, un asistente de ventas experto de Tecnovariedades D&S.

PERSONALIDAD:
- Amigable y cercano
- Profesional pero no formal
- Usa emojis apropiados
- Respuestas concisas (mÃ¡ximo 3-4 lÃ­neas)

PRODUCTOS:
- Laptops y computadoras
- Motos y vehÃ­culos
- Cursos digitales
- Megapacks de contenido

REGLAS:
1. EnfÃ³cate en BENEFICIOS, no en especificaciones
2. Si mencionan presupuesto, busca opciones en ese rango
3. Si piden fotos, confirma que las enviarÃ¡s
4. NUNCA inventes informaciÃ³n
5. NUNCA copies ejemplos literalmente

ESTILO:
- Usa viÃ±etas para listas
- Destaca precios con ğŸ’°
- Usa emojis relevantes
- Siempre ofrece ayuda adicional
`
```

### 2. Agregar Ejemplos de Entrenamiento

Edita `src/lib/sales-training-data.ts`:

```typescript
export const TRAINING_SCENARIOS = [
  {
    userMessage: "busco una laptop para diseÃ±o grÃ¡fico",
    botResponse: "Â¡Perfecto! Para diseÃ±o grÃ¡fico te recomiendo...",
    context: "laptop_design",
    intent: "product_search"
  },
  // Agregar mÃ¡s ejemplos aquÃ­
]
```

### 3. Configurar Respuestas Directas

Edita `src/lib/direct-response-handler.ts`:

```typescript
const DIRECT_RESPONSES = {
  greeting: [
    "Â¡Hola! ğŸ‘‹ Soy {botName}, Â¿en quÃ© puedo ayudarte?",
    "Â¡Buenas! ğŸ˜Š Â¿QuÃ© estÃ¡s buscando hoy?"
  ],
  gratitude: [
    "Â¡Con gusto! ğŸ˜Š Â¿Algo mÃ¡s en lo que pueda ayudarte?",
    "Â¡Para eso estoy! ğŸ™Œ Â¿Necesitas algo mÃ¡s?"
  ]
}
```

### 4. Ajustar Formato de Respuestas

Edita `src/lib/response-formatter.ts`:

```typescript
export class ResponseFormatter {
  static format(text: string): string {
    // Agregar emojis
    text = this.addEmojis(text)
    
    // Crear viÃ±etas
    text = this.createBulletPoints(text)
    
    // Destacar precios
    text = this.highlightPrices(text)
    
    return text
  }
}
```

---

## ğŸ”§ OptimizaciÃ³n

### 1. Analizar Conversaciones

```bash
# Ejecutar anÃ¡lisis
npm run analyze:llm
# o
mejorar-llm.bat
```

Esto generarÃ¡:
- `training-dataset.json` - Dataset de conversaciones reales
- `optimized-system-prompt.txt` - Prompt optimizado

### 2. Ajustar ParÃ¡metros

En `llm-config.json`:

```json
{
  "groq": {
    "temperature": 0.7,  // Creatividad (0.0 - 1.0)
    "maxTokens": 300,    // Longitud mÃ¡xima
    "topP": 0.9          // Diversidad de respuestas
  }
}
```

**Recomendaciones:**
- `temperature: 0.7` - Balance entre creatividad y consistencia
- `maxTokens: 300` - Respuestas concisas
- `topP: 0.9` - Buena diversidad

### 3. Optimizar Velocidad

```env
# Usar modelo mÃ¡s rÃ¡pido
GROQ_MODEL=llama-3.1-8b-instant

# Reducir tokens
GROQ_MAX_TOKENS=200

# Desactivar razonamiento avanzado
AI_USE_REASONING=false
```

### 4. Mejorar PrecisiÃ³n

1. **Agregar mÃ¡s ejemplos de entrenamiento**
   - Edita `src/lib/sales-training-data.ts`
   - Incluye casos reales de tu negocio

2. **Refinar detecciÃ³n de intenciones**
   - Edita `src/lib/product-intelligence-service.ts`
   - Agrega keywords especÃ­ficos

3. **Actualizar informaciÃ³n de productos**
   - MantÃ©n la BD actualizada
   - Usa descripciones claras y completas

---

## ğŸ› Troubleshooting

### Problema: El bot no responde

**SoluciÃ³n:**
1. Verifica que Groq estÃ© configurado:
   ```bash
   # En .env
   GROQ_API_KEY=tu_key_aqui
   AI_ENABLED=true
   ```

2. Revisa los logs:
   ```
   [AI] âŒ Error: API key no configurada
   ```

3. Prueba la conexiÃ³n:
   ```bash
   npx tsx scripts/test-llm-completo.ts
   ```

### Problema: Respuestas muy lentas

**SoluciÃ³n:**
1. Reduce tokens:
   ```env
   GROQ_MAX_TOKENS=200
   ```

2. Usa modelo mÃ¡s rÃ¡pido:
   ```env
   GROQ_MODEL=llama-3.1-8b-instant
   ```

3. Activa respuestas directas:
   ```typescript
   // En direct-response-handler.ts
   // Agregar mÃ¡s patrones de respuesta directa
   ```

### Problema: Respuestas incorrectas

**SoluciÃ³n:**
1. Revisa el system prompt
2. Agrega mÃ¡s ejemplos de entrenamiento
3. Actualiza informaciÃ³n de productos en BD
4. Verifica el contexto de conversaciÃ³n

### Problema: El bot inventa informaciÃ³n

**SoluciÃ³n:**
1. Refuerza las reglas en el system prompt:
   ```typescript
   NUNCA inventes informaciÃ³n que no tienes.
   Si no sabes algo, di "DÃ©jame verificar eso".
   ```

2. Usa temperatura mÃ¡s baja:
   ```json
   "temperature": 0.5
   ```

---

## âœ¨ Mejores PrÃ¡cticas

### 1. System Prompt

âœ… **Hacer:**
- Ser especÃ­fico sobre el rol y negocio
- Incluir ejemplos concretos
- Definir reglas claras
- Especificar el tono deseado

âŒ **Evitar:**
- Prompts muy largos (> 500 palabras)
- Instrucciones contradictorias
- Ser demasiado genÃ©rico

### 2. Ejemplos de Entrenamiento

âœ… **Hacer:**
- Usar conversaciones reales
- Incluir variedad de casos
- Actualizar regularmente
- Cubrir todos los productos

âŒ **Evitar:**
- Ejemplos inventados
- Casos muy especÃ­ficos
- Respuestas muy largas

### 3. Contexto de ConversaciÃ³n

âœ… **Hacer:**
- Mantener historial de 24h
- Guardar producto mencionado
- Recordar presupuesto del cliente
- Limpiar contexto antiguo

âŒ **Evitar:**
- Historial muy largo (> 10 mensajes)
- Contexto sin lÃ­mite de tiempo
- No limpiar memoria

### 4. Formato de Respuestas

âœ… **Hacer:**
- Usar emojis apropiados
- Crear viÃ±etas para listas
- Destacar precios
- Mantener respuestas concisas

âŒ **Evitar:**
- Demasiados emojis
- Respuestas muy largas
- Formato inconsistente

### 5. Monitoreo

âœ… **Hacer:**
- Revisar logs regularmente
- Analizar conversaciones
- Medir tiempos de respuesta
- Recopilar feedback

âŒ **Evitar:**
- Ignorar errores
- No actualizar el sistema
- No medir mÃ©tricas

---

## ğŸ“Š MÃ©tricas Clave

### Rendimiento
- **Tiempo de respuesta**: < 2 segundos
- **Tasa de Ã©xito**: > 95%
- **Uptime**: > 99%

### Calidad
- **PrecisiÃ³n de intenciones**: > 90%
- **SatisfacciÃ³n del cliente**: > 4.5/5
- **Tasa de escalamiento**: < 5%

### Uso
- **Mensajes procesados/dÃ­a**: Variable
- **Conversaciones activas**: Variable
- **Productos recomendados**: Variable

---

## ğŸ”— Enlaces Ãštiles

- [DocumentaciÃ³n de Groq](https://console.groq.com/docs)
- [Llama 3.1 Model Card](https://ai.meta.com/llama/)
- [GuÃ­a de Prompts](https://www.promptingguide.ai/)

---

## ğŸ“ Comandos RÃ¡pidos

```bash
# Iniciar sistema
npm run dev

# Test completo
npm run test:llm

# Analizar y mejorar
npm run analyze:llm

# Ver logs
npm run dev | grep "\[AI\]"

# Limpiar cachÃ©
npm run clean
```

---

## ğŸ†˜ Soporte

Si tienes problemas:

1. Revisa esta guÃ­a
2. Consulta `ESTADO_LLM_BOT_ACTUAL.md`
3. Ejecuta `test-llm.bat` para diagnosticar
4. Revisa los logs del servidor

---

**Ãšltima actualizaciÃ³n**: 2025-01-09
**VersiÃ³n**: 1.0.0

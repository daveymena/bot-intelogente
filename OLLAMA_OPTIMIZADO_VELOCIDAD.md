# âš¡ Ollama Optimizado para Velocidad

## ðŸŽ¯ Objetivo

Configurar Ollama para que las respuestas del bot sean **ultra rÃ¡pidas** (< 3 segundos) manteniendo buena calidad.

## âœ… ConfiguraciÃ³n Actual

Tu Ollama ya estÃ¡ funcionando en:
```
URL: https://bot-whatsapp-ollama.sqaoeo.easypanel.host
Modelo: gemma:2b
Estado: âœ… ACTIVO
```

## ðŸš€ Optimizaciones Implementadas

### 1. ConfiguraciÃ³n en `.env`

```env
# Ollama OPTIMIZADO
OLLAMA_BASE_URL=https://bot-whatsapp-ollama.sqaoeo.easypanel.host
OLLAMA_MODEL=gemma:2b
OLLAMA_ENABLED=true
OLLAMA_TIMEOUT=10000          # 10 segundos mÃ¡ximo
OLLAMA_MAX_TOKENS=300         # Respuestas concisas

# Ollama como prioridad (mÃ¡s rÃ¡pido que Groq)
AI_PROVIDER=ollama
AI_FALLBACK_ORDER=ollama,groq,openrouter
```

### 2. ParÃ¡metros de Rendimiento

El cÃ³digo ahora usa:
```typescript
{
  num_predict: 300,      // Tokens mÃ¡ximos (respuestas cortas)
  num_ctx: 2048,         // Contexto reducido (mÃ¡s rÃ¡pido)
  num_batch: 512,        // Procesamiento por lotes
  num_gpu: 1,            // Usar GPU si disponible
  num_thread: 4,         // Threads CPU
  temperature: 0.7,      // Balance creatividad/velocidad
  top_k: 40,            // Muestreo eficiente
  top_p: 0.9,           // Nucleus sampling
  repeat_penalty: 1.1    // Evitar repeticiones
}
```

### 3. Timeout Optimizado

- **Antes:** 30 segundos (muy lento)
- **Ahora:** 10 segundos (rÃ¡pido)
- **Fallback:** Si falla, usa Groq automÃ¡ticamente

## ðŸ“Š Velocidades Esperadas

### Con gemma:2b (Recomendado)
```
Saludo simple:        0.5 - 1.5 segundos  âš¡âš¡âš¡
Consulta producto:    1.0 - 2.5 segundos  âš¡âš¡
Respuesta detallada:  2.0 - 4.0 segundos  âš¡
```

### Con otros modelos
```
phi:2.7b:            1.0 - 3.0 segundos  âš¡âš¡
llama3.2:3b:         2.0 - 5.0 segundos  âš¡
llama3.1:8b:         4.0 - 8.0 segundos  (mÃ¡s lento)
```

## ðŸ§ª Probar Velocidad

```bash
npx tsx scripts/test-ollama-velocidad.ts
```

Este script te mostrarÃ¡:
- âœ… ConexiÃ³n con Ollama
- âš¡ Tiempo de respuesta real
- ðŸ“Š EstadÃ­sticas de rendimiento
- ðŸ’¡ Recomendaciones personalizadas

## ðŸŽ¯ Modelos Recomendados por Velocidad

### 1. gemma:2b (ULTRA RÃPIDO) âš¡âš¡âš¡
```bash
# En tu servidor Ollama
ollama pull gemma:2b
```
- **TamaÃ±o:** 2GB
- **Velocidad:** Excelente
- **Calidad:** Buena para conversaciones
- **Uso:** Respuestas rÃ¡pidas, saludos, consultas simples

### 2. phi:2.7b (MUY RÃPIDO) âš¡âš¡
```bash
ollama pull phi:2.7b
```
- **TamaÃ±o:** 2.7GB
- **Velocidad:** Muy buena
- **Calidad:** Muy buena
- **Uso:** Balance perfecto velocidad/calidad

### 3. llama3.2:3b (RÃPIDO) âš¡
```bash
ollama pull llama3.2:3b
```
- **TamaÃ±o:** 3GB
- **Velocidad:** Buena
- **Calidad:** Excelente
- **Uso:** Respuestas mÃ¡s elaboradas

## âš™ï¸ ConfiguraciÃ³n por Tipo de Respuesta

### Saludos y Respuestas Simples
```typescript
{
  max_tokens: 50,
  temperature: 0.7
}
// Tiempo esperado: < 1 segundo
```

### Consultas de Productos
```typescript
{
  max_tokens: 150,
  temperature: 0.7
}
// Tiempo esperado: 1-2 segundos
```

### Respuestas Detalladas
```typescript
{
  max_tokens: 300,
  temperature: 0.8
}
// Tiempo esperado: 2-3 segundos
```

## ðŸ”§ Optimizaciones Adicionales

### 1. En tu Servidor Ollama (Easypanel)

Verifica que tenga recursos suficientes:
```bash
# Memoria recomendada
RAM: 4GB mÃ­nimo (8GB ideal)
CPU: 2 cores mÃ­nimo (4 cores ideal)
GPU: Opcional pero mejora mucho
```

### 2. Variables de Entorno en Easypanel

Agrega estas variables en tu servicio Ollama:
```env
OLLAMA_NUM_PARALLEL=2
OLLAMA_MAX_LOADED_MODELS=1
OLLAMA_KEEP_ALIVE=5m
```

### 3. Usar GPU (Si disponible)

Si tu servidor tiene GPU:
```env
OLLAMA_GPU_LAYERS=35
```

## ðŸ“ˆ Monitoreo de Rendimiento

El sistema ahora registra tiempos de respuesta:
```
[Ollama] ðŸš€ Usando modelo: gemma:2b
[Ollama] âš¡ Respuesta en 1234ms
```

Busca estos logs para monitorear velocidad.

## ðŸŽ›ï¸ Ajustes Finos

### Si las respuestas son muy lentas (> 5 segundos)

1. **Cambiar a modelo mÃ¡s pequeÃ±o:**
   ```env
   OLLAMA_MODEL=gemma:2b
   ```

2. **Reducir tokens:**
   ```env
   OLLAMA_MAX_TOKENS=200
   ```

3. **Reducir contexto:**
   ```typescript
   num_ctx: 1024  // En lugar de 2048
   ```

### Si las respuestas son muy cortas

1. **Aumentar tokens:**
   ```env
   OLLAMA_MAX_TOKENS=400
   ```

2. **Ajustar temperatura:**
   ```typescript
   temperature: 0.8  // MÃ¡s creativo
   ```

## ðŸ”„ Sistema de Fallback

Si Ollama falla o es muy lento, el sistema automÃ¡ticamente usa:

1. **Ollama** (Primero - Local y rÃ¡pido)
2. **Groq** (Segundo - Cloud ultra rÃ¡pido)
3. **OpenRouter** (Tercero - Respaldo premium)

Configurado en:
```env
AI_FALLBACK_ORDER=ollama,groq,openrouter
```

## ðŸ’¡ Mejores PrÃ¡cticas

### 1. Usa Ollama para:
- âœ… Saludos y respuestas simples
- âœ… Consultas de productos
- âœ… Conversaciones normales
- âœ… Respuestas que no requieren razonamiento complejo

### 2. Usa Groq (fallback) para:
- âœ… AnÃ¡lisis complejos
- âœ… Razonamiento profundo
- âœ… Cuando Ollama falla
- âœ… Respuestas muy largas

### 3. Optimiza segÃºn uso:
```typescript
// ConversaciÃ³n normal
max_tokens: 150-200

// DescripciÃ³n de producto
max_tokens: 200-300

// AnÃ¡lisis complejo
max_tokens: 300-500 (usa Groq)
```

## ðŸ§ª Comandos de Prueba

### Test rÃ¡pido
```bash
npx tsx scripts/test-ollama-velocidad.ts
```

### Test de conversaciÃ³n real
```bash
npx tsx scripts/test-bot-conversacion.ts
```

### Ver logs en tiempo real
```bash
npm run dev
# Luego envÃ­a mensajes por WhatsApp
```

## ðŸ“Š MÃ©tricas de Ã‰xito

### Excelente âš¡âš¡âš¡
- Respuesta < 2 segundos
- Cliente no nota espera
- ConversaciÃ³n fluida

### Bueno âš¡âš¡
- Respuesta 2-4 segundos
- Espera tolerable
- Experiencia aceptable

### Mejorable âš¡
- Respuesta > 4 segundos
- Cliente puede impacientarse
- Considera optimizar

## ðŸŽ¯ ConfiguraciÃ³n Recomendada Final

```env
# .env
OLLAMA_BASE_URL=https://bot-whatsapp-ollama.sqaoeo.easypanel.host
OLLAMA_MODEL=gemma:2b
OLLAMA_ENABLED=true
OLLAMA_TIMEOUT=10000
OLLAMA_MAX_TOKENS=300

AI_PROVIDER=ollama
AI_FALLBACK_ENABLED=true
AI_FALLBACK_ORDER=ollama,groq,openrouter

GROQ_API_KEY=tu_key_aqui
GROQ_MODEL=llama-3.1-8b-instant
GROQ_TIMEOUT=5000
```

## âœ… Checklist de OptimizaciÃ³n

- [x] Ollama configurado y funcionando
- [x] Modelo pequeÃ±o y rÃ¡pido (gemma:2b)
- [x] Timeout optimizado (10 segundos)
- [x] Max tokens reducido (300)
- [x] ParÃ¡metros de rendimiento configurados
- [x] Sistema de fallback activo
- [x] Logs de tiempo de respuesta
- [ ] Probar con `test-ollama-velocidad.ts`
- [ ] Monitorear tiempos reales
- [ ] Ajustar segÃºn necesidad

## ðŸš€ PrÃ³ximos Pasos

1. **Ejecutar test de velocidad:**
   ```bash
   npx tsx scripts/test-ollama-velocidad.ts
   ```

2. **Probar en conversaciÃ³n real:**
   - EnvÃ­a mensajes por WhatsApp
   - Observa tiempos de respuesta
   - Ajusta si es necesario

3. **Monitorear logs:**
   ```bash
   npm run dev
   # Busca: [Ollama] âš¡ Respuesta en XXXms
   ```

4. **Optimizar segÃºn resultados:**
   - Si muy lento: modelo mÃ¡s pequeÃ±o
   - Si muy corto: aumentar tokens
   - Si falla mucho: verificar servidor

## ðŸ“ž Soporte

Si tienes problemas:

1. Verifica que Ollama estÃ© corriendo en Easypanel
2. Ejecuta el test de velocidad
3. Revisa los logs del servidor
4. Ajusta configuraciÃ³n segÃºn recomendaciones

---

**Â¡Ollama optimizado y listo para respuestas ultra rÃ¡pidas!** âš¡ðŸš€

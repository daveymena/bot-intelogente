# ‚úÖ Ollama Configurado como Fallback Ilimitado

## üéØ Configuraci√≥n Implementada

### Orden de Fallback (AI_FALLBACK_ORDER)

```
1. Groq (Principal) ‚Üí R√°pido, confiable, pero con l√≠mite de tokens
2. Ollama (Fallback) ‚Üí ILIMITADO, local, m√°s lento pero siempre disponible
```

## ü¶ô Ollama - Caracter√≠sticas

### Ventajas:
- ‚úÖ **Ilimitado** - Sin l√≠mite de tokens ni requests
- ‚úÖ **Gratis** - No cuesta nada
- ‚úÖ **Siempre disponible** - Servidor local/VPS
- ‚úÖ **Privado** - Los datos no salen de tu servidor

### Desventajas:
- ‚è±Ô∏è **M√°s lento** - Tarda 10-30 segundos en responder
- üß† **Menos inteligente** - Modelos m√°s peque√±os (gemma:2b)
- üíª **Requiere recursos** - Necesita servidor con RAM/CPU

## ‚öôÔ∏è Configuraci√≥n en .env

```env
# Ollama (IA local - HABILITADO como √∫ltimo fallback)
OLLAMA_BASE_URL=https://bot-whatsapp-ollama.sqaoeo.easypanel.host
OLLAMA_MODEL=gemma:2b
OLLAMA_ENABLED=true
OLLAMA_TIMEOUT=30000        # 30 segundos (aumentado para dar tiempo)
OLLAMA_MAX_TOKENS=400       # Respuestas completas

# Sistema de Fallback
AI_FALLBACK_ORDER=groq,ollama
```

## üîÑ C√≥mo Funciona el Fallback

### Escenario 1: Todo Normal
```
Cliente: "Tienes laptops?"
‚Üí Groq responde en 2 segundos ‚úÖ
```

### Escenario 2: Groq sin Tokens
```
Cliente: "Tienes laptops?"
‚Üí Groq falla (sin tokens) ‚ùå
‚Üí Ollama responde en 15 segundos ‚úÖ (ILIMITADO)
```

## üìä Tiempos de Respuesta Esperados

| Provider | Tiempo Normal | Timeout |
|----------|---------------|---------|
| Groq | 1-3 segundos | 15s |
| Ollama | 10-30 segundos | 30s |

## üß™ Probar Ollama

### 1. Verificar que Ollama est√© corriendo:
```bash
curl https://bot-whatsapp-ollama.sqaoeo.easypanel.host/api/tags
```

### 2. Probar respuesta:
```bash
npx tsx scripts/test-ollama.ts
```

### 3. Forzar uso de Ollama:
```bash
# Temporalmente deshabilitar Groq
GROQ_API_KEY="" npm run dev
```

## üöÄ Modelos Recomendados para Ollama

### Para Velocidad (Recomendado):
- `gemma:2b` - Muy r√°pido, respuestas b√°sicas (ACTUAL)
- `phi3:mini` - R√°pido, mejor calidad
- `tinyllama` - El m√°s r√°pido, calidad b√°sica

### Para Calidad (M√°s lento):
- `llama3.2:3b` - Balance velocidad/calidad
- `mistral:7b` - Mejor calidad, m√°s lento
- `llama3.1:8b` - Excelente calidad, muy lento

## üìù Cambiar Modelo de Ollama

1. Editar `.env`:
```env
OLLAMA_MODEL=phi3:mini
```

2. Descargar modelo en servidor Ollama:
```bash
# En el servidor donde corre Ollama
ollama pull phi3:mini
```

3. Reiniciar bot:
```bash
npm run dev
```

## ‚ö†Ô∏è Consideraciones Importantes

### Demora Humana:
- Ollama tarda m√°s, pero el bot ya tiene demora humana configurada (2-10s)
- El cliente no notar√° tanto la diferencia
- La demora hace que parezca m√°s natural

### Calidad de Respuestas:
- Ollama con gemma:2b es menos inteligente que Groq
- Pero sigue el mismo prompt del sistema
- Las respuestas ser√°n correctas pero quiz√°s menos naturales

### Cu√°ndo se Usar√°:
- Solo cuando Groq falle o se quede sin tokens
- Es un "salvavidas" para que el bot nunca deje de funcionar
- Garantiza respuestas ilimitadas 24/7

## ‚úÖ Ventajas de Esta Configuraci√≥n

1. **Nunca se cae** - Siempre hay un fallback disponible
2. **Optimizado** - Usa el m√°s r√°pido primero
3. **Econ√≥mico** - Ollama es gratis e ilimitado
4. **Transparente** - El cliente no nota el cambio de provider

## üîß Troubleshooting

### Ollama no responde:
```bash
# Verificar que est√© corriendo
curl https://bot-whatsapp-ollama.sqaoeo.easypanel.host/api/tags

# Ver logs del bot
npm run dev
# Buscar: [Ollama] en los logs
```

### Ollama muy lento:
```bash
# Cambiar a modelo m√°s r√°pido
OLLAMA_MODEL=tinyllama

# O reducir max_tokens
OLLAMA_MAX_TOKENS=200
```

### Ollama da respuestas malas:
```bash
# Cambiar a modelo mejor
OLLAMA_MODEL=llama3.2:3b

# O aumentar timeout
OLLAMA_TIMEOUT=60000
```

---

**Estado:** ‚úÖ Configurado y Listo
**Fecha:** 2025-11-04
**Beneficio:** Bot nunca se queda sin IA, siempre responde

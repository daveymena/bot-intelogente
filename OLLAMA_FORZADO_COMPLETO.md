# ü¶ô OLLAMA FORZADO COMPLETAMENTE

## ‚úÖ Cambios Aplicados

### 1. SearchAgent FORZADO a usar Ollama
- **Antes**: Intentaba b√∫squeda local primero
- **Ahora**: SIEMPRE usa Ollama para b√∫squedas
- `canHandleLocally()` retorna `false` SIEMPRE
- Implementado `handleWithAI()` obligatorio

### 2. Flujo de B√∫squeda con Ollama

```
Cliente: "Curso de Piano"
    ‚Üì
SearchAgent detecta mensaje
    ‚Üì
canHandleLocally() ‚Üí FALSE (forzado)
    ‚Üì
handleWithAI() ‚Üí Llama a Ollama
    ‚Üì
Ollama analiza: "Cliente busca: Curso de Piano"
    ‚Üì
Extrae keywords: ["curso", "piano"]
    ‚Üì
Busca en BD con keywords
    ‚Üì
Encuentra productos y responde
```

### 3. Configuraci√≥n .env

```bash
# Ollama habilitado
OLLAMA_ENABLED=true
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=gemma2:4b
OLLAMA_TIMEOUT=300000

# FORZAR uso de Ollama
FORCE_OLLAMA_ONLY=true
FORCE_AI_FOR_ALL=true
AI_FALLBACK_ORDER=ollama
```

### 4. Ventajas del Sistema Forzado

‚úÖ **Contexto Completo**: Ollama ve toda la conversaci√≥n
‚úÖ **Comprensi√≥n Natural**: Entiende "Curso de Piano" sin regex
‚úÖ **Memoria Conversacional**: Recuerda productos mencionados
‚úÖ **Gratis**: Sin l√≠mites de tokens
‚úÖ **Coherencia**: Respuestas m√°s naturales

### 5. C√≥mo Funciona handleWithAI()

```typescript
async handleWithAI(message: string, memory: SharedMemory) {
  // 1. Construir contexto
  const conversationContext = memory.messages
    .slice(-5)
    .map(m => `${m.role}: ${m.content}`)
    .join('\n');
  
  // 2. Llamar a Ollama
  const aiResponse = await AIMultiProvider.generateCompletion([
    { role: 'system', content: 'Eres experto en ventas...' },
    { role: 'user', content: `Cliente: "${message}"` }
  ]);
  
  // 3. Extraer keywords de respuesta de Ollama
  const keywords = this.extractKeywordsFromAI(aiResponse.content);
  
  // 4. Buscar en BD
  const products = await this.simpleSearch(keywords.join(' '));
  
  // 5. Responder
  return this.showProductList(products);
}
```

### 6. Extracci√≥n Inteligente de Keywords

Ollama responde algo como:
```
"El cliente est√° buscando un curso de piano. 
Palabras clave: curso, piano"
```

El sistema extrae:
- Busca patrones: "palabras clave:", "busca:", "keywords:"
- Si no encuentra, usa el mensaje original
- Filtra palabras cortas (< 3 letras)

## üß™ Probar Ahora

```bash
# 1. Aseg√∫rate que Ollama est√© corriendo
ollama serve

# 2. Verifica el modelo
ollama list

# 3. Reinicia el bot
npm run dev

# 4. Prueba en WhatsApp
"Curso de Piano"
"Busco laptop para dise√±o"
"Quiero ver motos"
```

## üìä Logs Esperados

```
[SearchAgent] ü¶ô FORZANDO uso de Ollama para TODAS las b√∫squedas
[SearchAgent] ü¶ô Usando Ollama para b√∫squeda inteligente
[Ollama] üöÄ Usando modelo: gemma2:4b
[Ollama] ‚ö° Respuesta en 3500ms
[SearchAgent] ü¶ô Ollama respondi√≥: El cliente busca un curso de piano...
[SearchAgent] üîë Keywords extra√≠das por Ollama: curso, piano
[SearchAgent] üì¶ Encontrados 1 productos
[SearchAgent] ‚úÖ 1 producto encontrado por Ollama - Mostrando con foto
```

## ‚ö†Ô∏è Importante

- **Ollama DEBE estar corriendo**: `ollama serve`
- **Modelo descargado**: `ollama pull gemma2:4b`
- **Puerto correcto**: 11434 (default)
- **Timeout alto**: 300 segundos (5 minutos)

## üéØ Resultado Esperado

Ahora el bot:
1. ‚úÖ Usa Ollama para TODAS las b√∫squedas
2. ‚úÖ Entiende contexto conversacional
3. ‚úÖ Encuentra "Curso de Piano" correctamente
4. ‚úÖ Mantiene coherencia en respuestas
5. ‚úÖ NO pierde memoria entre mensajes

## üöÄ Siguiente Paso

Si funciona bien, podemos:
1. Optimizar el prompt de Ollama
2. Mejorar extracci√≥n de keywords
3. Agregar cach√© de respuestas frecuentes
4. Implementar aprendizaje de patrones

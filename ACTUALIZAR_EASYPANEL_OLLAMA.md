# üöÄ ACTUALIZAR EASYPANEL CON OLLAMA OPTIMIZADO

## Variables a Actualizar en Easypanel

Ve a tu proyecto en Easypanel ‚Üí Settings ‚Üí Environment Variables y actualiza estas variables:

### 1. Cambiar Prioridad de IA

```env
# Cambiar de Groq a Ollama como principal
AI_PROVIDER=ollama
DEFAULT_AI_PROVIDER=ollama
AI_FALLBACK_ORDER=ollama,groq
```

### 2. Configurar Ollama

```env
# URL de Ollama en Easypanel
OLLAMA_BASE_URL=https://bot-whatsapp-ollama.sqaoeo.easypanel.host

# O si est√° en el mismo proyecto (m√°s r√°pido):
OLLAMA_BASE_URL=http://ollama:11434

# Modelo m√°s potente
OLLAMA_MODEL=qwen2.5:7b
OLLAMA_ENABLED=true
OLLAMA_TIMEOUT=30000
OLLAMA_MAX_TOKENS=800
```

### 3. Optimizar Groq (Fallback)

```env
# Usar modelos peque√±os primero
GROQ_MODEL=llama-3.1-8b-instant
GROQ_FALLBACK_MODELS=llama-3.1-8b-instant,gemma2-9b-it
```

## Verificar que Ollama Funciona

### Opci√≥n 1: Desde Easypanel Console

```bash
# Conectar al contenedor de Ollama
docker exec -it ollama ollama list

# Deber√≠a mostrar los modelos instalados
```

### Opci√≥n 2: Desde URL Externa

```bash
curl https://bot-whatsapp-ollama.sqaoeo.easypanel.host/api/tags
```

Deber√≠a retornar JSON con los modelos disponibles.

## Instalar Modelo Recomendado

Si `llama3.1:8b` no est√° instalado:

```bash
# Conectar a Easypanel Console
docker exec -it ollama ollama pull qwen2.5:7b

# O modelo m√°s peque√±o pero r√°pido:
docker exec -it ollama ollama pull llama3.2:3b
```

## Configuraci√≥n Recomendada

### Para M√°ximo Rendimiento (Ollama en mismo proyecto)

```env
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=qwen2.5:7b
AI_PROVIDER=ollama
```

**Ventajas:**
- ‚úÖ M√°s r√°pido (red interna)
- ‚úÖ Sin latencia de internet
- ‚úÖ M√°s seguro

### Para M√°xima Compatibilidad (URL externa)

```env
OLLAMA_BASE_URL=https://bot-whatsapp-ollama.sqaoeo.easypanel.host
OLLAMA_MODEL=qwen2.5:7b
AI_PROVIDER=ollama
```

**Ventajas:**
- ‚úÖ Funciona aunque est√©n en proyectos diferentes
- ‚úÖ M√°s f√°cil de debuggear
- ‚úÖ Accesible desde fuera

## Despu√©s de Actualizar

1. **Reiniciar el servicio** en Easypanel
2. **Verificar logs** para ver que usa Ollama:
   ```
   [Ollama] Usando modelo qwen2.5:7b
   ‚úÖ B√∫squeda local: 4 productos encontrados (sin usar IA)
   ```

3. **Probar con un mensaje:**
   - Env√≠a: "Hola, tienes parlantes?"
   - Deber√≠a responder sin errores de rate limit

## Troubleshooting

### Error: "Cannot connect to Ollama"

**Soluci√≥n 1:** Verificar que Ollama est√© corriendo
```bash
docker ps | grep ollama
```

**Soluci√≥n 2:** Usar URL interna si est√°n en mismo proyecto
```env
OLLAMA_BASE_URL=http://ollama:11434
```

**Soluci√≥n 3:** Verificar que el puerto est√© expuesto en Easypanel

### Error: "Model not found"

**Soluci√≥n:** Instalar el modelo
```bash
docker exec -it ollama ollama pull qwen2.5:7b
```

### Ollama muy lento

**Soluci√≥n 1:** Usar modelo m√°s peque√±o
```env
OLLAMA_MODEL=llama3.2:3b
```

**Soluci√≥n 2:** Aumentar recursos del contenedor en Easypanel
- CPU: M√≠nimo 2 cores
- RAM: M√≠nimo 4GB

### Sigue usando Groq en lugar de Ollama

**Soluci√≥n:** Verificar variables de entorno
```env
AI_PROVIDER=ollama  # NO groq
DEFAULT_AI_PROVIDER=ollama  # NO groq
```

## Monitoreo

### Ver qu√© proveedor est√° usando:

En los logs del bot busca:
- `[Ollama]` = Usando Ollama ‚úÖ
- `[Groq Rotator]` = Usando Groq (fallback) ‚ö†Ô∏è
- `‚úÖ B√∫squeda local` = Sin usar IA (√≥ptimo) üöÄ

### Estad√≠sticas esperadas:

- 90% de consultas: B√∫squeda local (0 tokens)
- 8% de consultas: Ollama (0 tokens, ilimitado)
- 2% de consultas: Groq (solo si Ollama falla)

## Resultado Final

‚úÖ **IA ilimitada** con Ollama
‚úÖ **Sin l√≠mites de tokens**
‚úÖ **Fallback autom√°tico** a Groq si falla
‚úÖ **90% de consultas sin IA** (b√∫squeda local)
‚úÖ **Ahorro del 95% en costos** de API

---

**Archivo de referencia completo:** `VARIABLES_ENTORNO_EASYPANEL_COMPLETAS.env`

/**
 * ğŸ¯ OLLAMA ORCHESTRATOR PROFESSIONAL
 * Sistema simple y rÃ¡pido con gemma2:2b
 */

import { db } from './db'

interface Message {
  role: 'system' | 'user' | 'assistant'
  content: string
}

interface OrchestratorResponse {
  message: string
  source: 'ollama' | 'groq' | 'local'
  confidence: number
  products?: any[]
}

export class OllamaProfessionalOrchestrator {
  private static config = {
    url: process.env.OLLAMA_BASE_URL || process.env.OLLAMA_URL || 'https://davey-ollama2.mapf5v.easypanel.host',
    model: process.env.OLLAMA_MODEL || 'llama3.2:3b', // âš¡ MÃ¡s rÃ¡pido: 527ms
    modelSecondary: 'gemma2:2b', // ğŸ¥ˆ Fallback: 670ms
    modelTertiary: 'llama3.1:8b', // ğŸ¥‰ Fallback final: 1263ms
    timeout: parseInt(process.env.OLLAMA_TIMEOUT || '30000') // âœ… Reducido a 30s (3x promedio)
  }

  // CachÃ© de respuestas rÃ¡pidas
  private static quickResponses: Record<string, string> = {
    'hola': 'Â¡Hola! ğŸ˜Š Bienvenido a Tecnovariedades D&S. Â¿En quÃ© puedo ayudarte?',
    'hi': 'Â¡Hola! ğŸ˜Š Bienvenido a Tecnovariedades D&S. Â¿En quÃ© puedo ayudarte?',
    'gracias': 'Â¡Con gusto! ğŸ˜Š Â¿Necesitas algo mÃ¡s?',
    'ok': 'Â¡Perfecto! ğŸ˜Š Â¿Algo mÃ¡s en lo que pueda ayudarte?',
    'sÃ­': 'Â¡Excelente! ğŸ˜Š',
    'no': 'Entendido. Â¿Algo mÃ¡s? ğŸ˜Š'
  }

  /**
   * Procesar mensaje
   */
  static async processMessage(
    userMessage: string,
    userId: string,
    conversationHistory: Message[] = []
  ): Promise<OrchestratorResponse> {
    // Verificar cachÃ©
    const lowerMsg = userMessage.toLowerCase().trim()
    if (this.quickResponses[lowerMsg]) {
      console.log('[Orchestrator] âš¡ Respuesta desde cachÃ©')
      return {
        message: this.quickResponses[lowerMsg],
        source: 'ollama',
        confidence: 100
      }
    }

    // Buscar productos
    const products = await this.searchProducts(userMessage, userId)
    console.log(`[Orchestrator] ğŸ” Productos encontrados: ${products.length}`)

    // Construir prompt
    const systemPrompt = this.buildPrompt(products)
    const messages = [
      { role: 'system' as const, content: systemPrompt },
      ...conversationHistory.slice(-8),
      { role: 'user' as const, content: userMessage }
    ]

    // Llamar Ollama
    try {
      const response = await this.callOllama(messages)
      return {
        message: response,
        source: 'ollama',
        confidence: 85,
        products: products.length > 0 ? products : undefined
      }
    } catch (error) {
      console.log('[Orchestrator] âŒ Ollama fallÃ³, usando bot local')
      return this.localResponse(userMessage, products)
    }
  }

  /**
   * Buscar productos usando el sistema inteligente
   */
  private static async searchProducts(query: string, userId: string): Promise<any[]> {
    try {
      // Usar el sistema de bÃºsqueda inteligente
      const { intelligentProductSearch } = await import('./intelligent-product-search')
      
      const result = await intelligentProductSearch({
        userMessage: query,
        previousProducts: [],
        conversationHistory: []
      })

      if (!result) return []

      // Si es consulta general, devolver mÃºltiples productos
      if (result.isGeneralQuery && result.products) {
        return result.products.slice(0, 3)
      }

      // Si es consulta especÃ­fica, devolver solo ese producto
      if (result.product) {
        return [result.product]
      }

      return []
    } catch (error) {
      console.error('[Orchestrator] Error en bÃºsqueda inteligente:', error)
      return []
    }
  }

  /**
   * Construir prompt
   */
  private static buildPrompt(products: any[]): string {
    let prompt = `ğŸ‡ªğŸ‡¸ IDIOMA OBLIGATORIO: ESPAÃ‘OL (COLOMBIA) ğŸ‡ªğŸ‡¸
âš ï¸ NUNCA RESPONDAS EN INGLÃ‰S - SOLO ESPAÃ‘OL âš ï¸

Eres Alex, vendedor colombiano de Tecnovariedades D&S por WhatsApp.

ğŸš¨ REGLA CRÃTICA DE IDIOMA:
- SIEMPRE responde en ESPAÃ‘OL (Colombia)
- NUNCA uses inglÃ©s, ni una sola palabra
- Si el cliente pregunta en inglÃ©s, responde en ESPAÃ‘OL
- Eres un vendedor colombiano, NO un asistente genÃ©rico de IA
- NO digas "I understand", "Here's why", "Unfortunately" (INGLÃ‰S PROHIBIDO)

IDENTIDAD:
- Trabajas para: Tecnovariedades D&S
- Vendes: Productos reales de nuestro catÃ¡logo
- NO eres ChatGPT, Claude, ni asistente genÃ©rico
- Eres un VENDEDOR PROFESIONAL colombiano

REGLAS:
- Lee el historial completo
- Si dice "opciÃ³n 2" â†’ Busca quÃ© productos YA mostraste
- NO repitas el saludo
- MÃ¡ximo 4 lÃ­neas
- Emojis sutiles ğŸ˜Š
- SIEMPRE en ESPAÃ‘OL

AGENTES DEL SISTEMA:
- Agente de bÃºsqueda: Busca productos en BD
- Agente de pagos: Genera links con MercadoPago/PayPal API
- Agente de fotos: EnvÃ­a imÃ¡genes automÃ¡ticamente
- TÃš solo coordinas

PAGOS:
- Si dice "generar link": "Perfecto ğŸ™Œ Enseguida genero tu enlace..."
- El agente de pagos genera el link real
- NO inventes links

`

    if (products.length > 0) {
      prompt += `\nPRODUCTOS REALES:\n`
      products.slice(0, 3).forEach((p, i) => {
        prompt += `${i + 1}. ${p.name} - $${p.price.toLocaleString('es-CO')} COP\n`
      })
      prompt += `\nMuestra estos productos. NO inventes otros.\n`
    }

    return prompt
  }

  /**
   * Llamar Ollama con fallback automÃ¡tico entre modelos
   */
  private static async callOllama(messages: Message[]): Promise<string> {
    const prompt = messages.map(m => {
      if (m.role === 'system') return m.content
      if (m.role === 'user') return `Cliente: ${m.content}`
      return `Alex: ${m.content}`
    }).join('\n\n') + '\n\nAlex: '

    // Intentar con modelo primario (mÃ¡s rÃ¡pido)
    try {
      console.log(`[Orchestrator] âš¡ Intentando con ${this.config.model}`)
      return await this.callOllamaModel(prompt, this.config.model)
    } catch (error) {
      console.log(`[Orchestrator] âš ï¸ ${this.config.model} fallÃ³, probando ${this.config.modelSecondary}`)
      
      // Intentar con modelo secundario
      try {
        return await this.callOllamaModel(prompt, this.config.modelSecondary)
      } catch (error2) {
        console.log(`[Orchestrator] âš ï¸ ${this.config.modelSecondary} fallÃ³, probando ${this.config.modelTertiary}`)
        
        // Ãšltimo intento con modelo terciario
        return await this.callOllamaModel(prompt, this.config.modelTertiary)
      }
    }
  }

  /**
   * Llamar Ollama con un modelo especÃ­fico
   */
  private static async callOllamaModel(prompt: string, model: string): Promise<string> {
    const controller = new AbortController()
    const timeoutId = setTimeout(() => controller.abort(), this.config.timeout)

    try {
      const response = await fetch(`${this.config.url}/api/generate`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          model,
          prompt,
          stream: false,
          options: {
            temperature: 0.6,
            num_predict: 150,
            repeat_penalty: 1.2
          }
        }),
        signal: controller.signal
      })

      clearTimeout(timeoutId)

      if (!response.ok) {
        throw new Error(`Ollama HTTP ${response.status}`)
      }

      const data = await response.json()
      return data.response || ''
    } catch (error: any) {
      clearTimeout(timeoutId)
      throw error
    }
  }

  /**
   * Respuesta local (fallback)
   */
  private static localResponse(message: string, products: any[]): OrchestratorResponse {
    const lowerMsg = message.toLowerCase()

    if (products.length > 0) {
      let response = 'Â¡Perfecto! ğŸ˜Š Tengo:\n\n'
      products.slice(0, 3).forEach((p, i) => {
        response += `${i + 1}. ${p.name} - $${p.price.toLocaleString('es-CO')} COP\n`
      })
      response += '\nÂ¿CuÃ¡l te interesa?'
      
      return { message: response, source: 'local', confidence: 70, products }
    }

    // Respuestas genÃ©ricas
    if (lowerMsg.includes('pago') || lowerMsg.includes('pagar')) {
      return {
        message: 'Puedes pagar con MercadoPago, PayPal, Nequi o Daviplata. Â¿CuÃ¡l prefieres? ğŸ˜Š',
        source: 'local',
        confidence: 90
      }
    }

    return {
      message: 'ğŸ˜Š Â¿En quÃ© puedo ayudarte? Tenemos laptops, cursos y megapacks.',
      source: 'local',
      confidence: 60
    }
  }

  /**
   * Verificar disponibilidad
   */
  static async isAvailable(): Promise<boolean> {
    try {
      const response = await fetch(`${this.config.url}/api/tags`, {
        signal: AbortSignal.timeout(3000)
      })
      return response.ok
    } catch {
      return false
    }
  }
}

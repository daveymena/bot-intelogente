import Groq from 'groq-sdk';
import dotenv from 'dotenv';
import fetch from 'node-fetch';

dotenv.config();

export interface AIDecision {
  action: 'show_product' | 'show_payment' | 'handle_objection' | 'answer_question' | 'greet' | 'farewell' | 'general_inquiry';
  selectedProductIndex: number | null; // Usaremos √≠ndice para ser m√°s robustos
  reasoning: string;
  emotionalTone: 'enthusiastic' | 'cautious' | 'skeptical' | 'neutral';
  additionalContext: string;
}

/**
 * Analiza el mensaje con IA (Ollama primero, Groq como fallback)
 */
export async function analyzeWithAI(
  message: string,
  conversationHistory: Array<{ role: string; content: string }>,
  availableProducts: any[]
): Promise<AIDecision> {
  
  const productsList = availableProducts.map((p, index) => 
    `${index}. ${p.name} ($${p.price} COP): ${p.description?.substring(0, 150)}...`
  ).join('\n');

  const prompt = `Eres un experto asesor de ventas de "Tecnovariedades D&S". Tu objetivo es analizar el mensaje del cliente y decidir qu√© acci√≥n tomar, priorizando siempre ayudar al cliente y cerrar la venta de forma natural.

HISTORIAL DE CONVERSACI√ìN:
${conversationHistory.slice(-6).map(h => `${h.role === 'user' ? 'Cliente' : 'Asistente'}: ${h.content}`).join('\n')}

MENSAJE ACTUAL DEL CLIENTE:
"${message}"

PRODUCTOS EN CAT√ÅLOGO:
${productsList}

DECISIONES POSIBLES:
1. show_product: El cliente busca un producto, pregunta por opciones o quiere ver qu√© hay.
2. show_payment: El cliente expl√≠citamente quiere pagar, pide datos de pago o dice "s√≠" al ofrecimiento de compra.
3. handle_objection: El cliente tiene una duda, queja o dice que es caro/necesita pensarlo.
4. answer_question: Pregunta t√©cnica o espec√≠fica que no es una compra directa.
5. greet: Es solo un saludo inicial (Hola, buenos d√≠as).
6. farewell: El cliente se despide.
7. general_inquiry: No encaja en lo anterior.

INSTRUCCIONES:
- Analiza si el cliente se refiere a un producto espec√≠fico del cat√°logo anterior.
- Si el cliente dice "s√≠", "dale", "me interesa" despu√©s de que le mostraste un producto, decide 'show_payment'.
- Si el cliente responde con un m√©todo de entrega (digital, recoger, etc.), decide 'show_payment'.
- Selecciona el √≠ndice del producto si el cliente lo menciona o si est√°s recomendando uno.

RESPONDE √öNICAMENTE CON UN OBJETO JSON V√ÅLIDO. NO incluyas explicaciones ni markdown.

JSON:
{
  "action": "show_product | show_payment | handle_objection | answer_question | greet | farewell | general_inquiry",
  "selectedProductIndex": index_of_product_or_null,
  "reasoning": "short explanation",
  "emotionalTone": "enthusiastic | cautious | skeptical | neutral",
  "additionalContext": "short greeting or transition phrase in Spanish"
}`;

  try {
    // 1. Intentar con Ollama (Local)
    console.log('ü¶ô Intentando analizar con Ollama...');
    const result = await queryOllama(prompt);
    if (result) return result;
  } catch (error) {
    console.log('‚ö†Ô∏è Ollama no disponible o fall√≥:', (error as Error).message);
  }

  // 2. Fallback a Groq
  try {
    console.log('‚ö° Fallback a Groq...');
    return await queryGroq(prompt);
  } catch (error) {
    console.error('‚ùå Error fatal en an√°lisis de IA:', error);
    // Fallback de emergencia a algo que el bot pueda manejar
    return {
      action: 'general_inquiry',
      selectedProductIndex: null,
      reasoning: 'Error en IA, fallback a consulta general',
      emotionalTone: 'neutral',
      additionalContext: ''
    };
  }
}

async function queryOllama(prompt: string): Promise<AIDecision | null> {
  const url = process.env.OLLAMA_URL || 'http://localhost:11434/api/generate';
  try {
    const response = await fetch(url, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: process.env.OLLAMA_MODEL || 'gemma2:2b',
        prompt: prompt,
        stream: false,
        format: 'json'
      })
    });

    if (!response.ok) return null;
    const data: any = await response.json();
    return JSON.parse(data.response);
  } catch {
    return null;
  }
}

async function queryGroq(prompt: string): Promise<AIDecision> {
  const groq = new Groq({ apiKey: process.env.GROQ_API_KEY });
  const completion = await groq.chat.completions.create({
    messages: [{ role: 'user', content: prompt }],
    model: process.env.GROQ_MODEL || 'llama-3.1-8b-instant',
    response_format: { type: 'json_object' }
  });

  const content = completion.choices[0]?.message?.content;
  if (!content) throw new Error('Respuesta de Groq vac√≠a');
  return JSON.parse(content);
}

# ğŸ¤– CONFIGURAR OLLAMA COMO IA PRINCIPAL (GRATIS)

## ğŸ¯ Â¿POR QUÃ‰ OLLAMA?

- âœ… **100% GRATIS** - Sin lÃ­mites de uso
- âœ… **Local** - No depende de internet
- âœ… **RÃ¡pido** - Respuestas en segundos
- âœ… **Privado** - Datos no salen de tu servidor
- âŒ Groq - Tiene lÃ­mites de API (6000 tokens/min)

## ğŸ“Š PRIORIDAD DEL SISTEMA

```
1ï¸âƒ£ Ollama (Local, GRATIS) â† PRINCIPAL
    â†“ (si falla)
2ï¸âƒ£ Groq (Cloud, con lÃ­mites) â† FALLBACK
    â†“ (si falla)
3ï¸âƒ£ Plantillas locales â† ÃšLTIMO RECURSO
```

## ğŸš€ INSTALACIÃ“N DE OLLAMA

### Windows:
```bash
# Descargar desde: https://ollama.ai/download
# O usar winget:
winget install Ollama.Ollama
```

### Linux:
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

### macOS:
```bash
brew install ollama
```

## ğŸ“¦ INSTALAR MODELO RECOMENDADO

```bash
# Modelo ligero y rÃ¡pido (3B parÃ¡metros)
ollama pull llama3.2:3b

# O modelo mÃ¡s potente (8B parÃ¡metros)
ollama pull llama3.1:8b
```

## â–¶ï¸ INICIAR OLLAMA

```bash
# Iniciar servidor Ollama
ollama serve
```

**IMPORTANTE:** Deja esta terminal abierta mientras el bot estÃ© corriendo.

## âœ… VERIFICAR QUE FUNCIONA

```bash
# 1. Verificar que Ollama estÃ¡ corriendo
curl http://localhost:11434/api/tags

# Debe responder con lista de modelos instalados

# 2. Probar generaciÃ³n de texto
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2:3b",
  "prompt": "Hola, Â¿cÃ³mo estÃ¡s?",
  "stream": false
}'
```

## âš™ï¸ CONFIGURACIÃ“N EN .ENV

```bash
# Ollama (opcional, usa valores por defecto)
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b

# Groq (solo como fallback)
GROQ_API_KEY=tu_api_key_aqui
```

## ğŸ§ª PROBAR EL SISTEMA

```bash
# 1. Ejecutar test
npx tsx scripts/test-bot-usa-bd-ollama.ts

# Debe mostrar:
# âœ… Ollama disponible
# âœ… Modelos: llama3.2:3b
# âœ… Sistema hÃ­brido: Funcionando

# 2. Reiniciar bot
npm run dev

# Debe mostrar:
# [Baileys] âœ… Ollama disponible (GRATIS)
# [Baileys] ğŸ¯ Prioridad: 1ï¸âƒ£ Ollama â†’ 2ï¸âƒ£ Groq â†’ 3ï¸âƒ£ Plantillas
```

## ğŸ“Š LOGS ESPERADOS

Cuando el bot procesa un mensaje:

```
âœ… [OllamaFirst] ğŸ¤– Intentando con Ollama (local, gratis)...
âœ… [OllamaFirst] âœ… Respuesta generada con Ollama
âœ… [Baileys] âœ… Respuesta generada con sistema hÃ­brido (BD + IA)
```

Si Ollama no estÃ¡ disponible:

```
âš ï¸ [OllamaFirst] âš ï¸ Ollama no disponible: Connection refused
âš ï¸ [OllamaFirst] ğŸŒ Usando Groq como fallback...
âœ… [OllamaFirst] âœ… Respuesta generada con Groq (fallback)
```

## ğŸ”§ SOLUCIÃ“N DE PROBLEMAS

### Problema 1: "Connection refused"

```bash
# Verificar que Ollama estÃ¡ corriendo
ps aux | grep ollama

# Si no estÃ¡ corriendo, iniciarlo
ollama serve
```

### Problema 2: "Model not found"

```bash
# Listar modelos instalados
ollama list

# Si no estÃ¡ el modelo, instalarlo
ollama pull llama3.2:3b
```

### Problema 3: Ollama muy lento

```bash
# Usar modelo mÃ¡s ligero
ollama pull llama3.2:1b

# Actualizar .env
OLLAMA_MODEL=llama3.2:1b
```

### Problema 4: Puerto ocupado

```bash
# Cambiar puerto de Ollama
OLLAMA_HOST=0.0.0.0:11435 ollama serve

# Actualizar .env
OLLAMA_URL=http://localhost:11435
```

## ğŸ’° COMPARACIÃ“N DE COSTOS

| Proveedor | Costo | LÃ­mites | Velocidad |
|-----------|-------|---------|-----------|
| **Ollama** | ğŸ†“ GRATIS | âˆ Sin lÃ­mites | âš¡ RÃ¡pido (local) |
| Groq | ğŸ†“ Gratis con lÃ­mites | 6000 tokens/min | âš¡âš¡ Muy rÃ¡pido |
| OpenAI | ğŸ’° $0.002/1K tokens | SegÃºn plan | âš¡ Medio |
| Claude | ğŸ’° $0.003/1K tokens | SegÃºn plan | âš¡ Medio |

## ğŸ¯ RECOMENDACIÃ“N

**Para producciÃ³n:**
1. Usa Ollama como principal (gratis, sin lÃ­mites)
2. Configura Groq como fallback (por si Ollama falla)
3. Las plantillas locales son el Ãºltimo recurso

**Ventajas:**
- âœ… 99% de las respuestas serÃ¡n con Ollama (gratis)
- âœ… Solo usarÃ¡s Groq si Ollama falla (raro)
- âœ… Nunca te quedarÃ¡s sin respuestas

## ğŸ“ CHECKLIST FINAL

- [ ] Ollama instalado
- [ ] Modelo descargado (`ollama pull llama3.2:3b`)
- [ ] Ollama corriendo (`ollama serve`)
- [ ] Variables en .env configuradas
- [ ] Test ejecutado exitosamente
- [ ] Bot reiniciado
- [ ] Logs muestran "Ollama disponible (GRATIS)"

## ğŸš€ SIGUIENTE PASO

Una vez configurado Ollama:

```bash
# Reiniciar el bot
npm run dev

# Probar con WhatsApp
# Enviar: "busco un portÃ¡til para diseÃ±o"

# Verificar logs:
# Debe usar Ollama (no Groq)
```

---

**Fecha:** 25 de noviembre de 2025  
**Estado:** âœ… Ollama configurado como IA principal  
**Costo:** ğŸ†“ 100% GRATIS
